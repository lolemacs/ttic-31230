\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\bigskip
\centerline{\bf AlphaZero Problems.}

\bigskip
\bigskip
{\bf Backgound.}
A version of AlphaZero can be defined as follows.

\medskip
To select a move in the game, first construct a search tree over possible moves to evaluate options.

\medskip
The tree is grown by running ``simulations''.  Each simulation descends into the tree from the root selecting a move from each position
until the selected move has not been explored before.  When the simulation reaches an unexplored move it expands the tree by adding a node for that move.
Each simulation returns the value $V_\Phi(s)$ for the newly added node $s$.

\medskip
Each node in the search tree represents a board position $s$ and stores the following information which can be initialized
by running the value and policy networks on position $s$.

\begin{itemize}
\item $V_\Phi(s)$ --- the value network value for the position $s$.
\item For each legal move $a$ from $s$, the policy network probability $\pi_\Phi(s,a)$.
\item For each legal move $a$ from $s$, the number $N(s,a)$ of simulations that have taken move $a$ from $s$. This is initially zero.
\item For each legal move $a$ from $s$ with $N(s,a) > 0$, the average $\hat{\mu}(s,a)$ of the values of the simulations that have
  taken move $a$ from position $s$.
\end{itemize}

\medskip
In descending into the tree, simulations select the move $\argmax_a U(s,a)$ where we have
$$U(s,a) =  \left\{\begin{array}{ll}\lambda \pi_\Phi(s,a) &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + \lambda \pi_\Phi(s,a)/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1)$$

\medskip
When the search is completed, we must select a move from the root position.  For this we use a post-search stochastic policy
$$\pi_{s_{\mathrm{root}}}(a) \propto N(s_{\mathrm{root}},a)^\beta\;\;\;\;\;(2)$$
where $\beta$ is temperature hyperparameter.

\medskip
For training we construct

\medskip
\centerline{a replay buffer of triples $(s_{\mathrm{root}},\pi_{s_{\mathrm{root}}},z)$ \hspace{2em}$(3)$}

\medskip
accumulated from self play where $s_{\mathrm{root}}$ is a root position from a search during a game,
$\pi_{s_{\mathrm{root}}}$ is the post-search policy constructed for $s_{\mathrm{root}}$, and $z$ is the outcome of that game.

\medskip
Training is then done by SGD on the following objective function.

$$\Phi^*\; = \;\argmin_\Phi \;E_{(s,\pi,z) \sim \mathrm{Replay},\;a \sim \pi}\;\left(\begin{array}{l} (V_\Phi(s) - z)^2 \\ \\ - \lambda_1\log \pi_\Phi(a|s) \\ \\ + \lambda_2||\Phi||^2\end{array}\right)\;\;\;\;(4)$$

\bigskip
{\bf Problem 1.}
    
\medskip
(a) Consider the case of $\lambda < 1$ vs. $\lambda > 1$ in (1).  Which value of $\lambda$ leads to more diversity in the choice of actions during different simulations?  Explain your answer.

\medskip
(b) Which of $\lambda < 1$ or $\lambda > 1$ in (1) is more consistent with viewing $U(s,a)$ as a form of ``upper bound'' in the upper confidence bound (UCB) bandit algorithm?  Explain your answer.

\bigskip
{\bf Problem 2}
We consider replacing the policy network $\pi_\Phi$ with a $Q$-value network $Q_\Phi$ so that each node $s$ stores the $Q$-values $Q_\Phi(s,a)$ rather than
the policy probabilities $\pi_\Phi(s,a)$.  We then replace (1) with
$$U(s,a) =  \left\{\begin{array}{ll}(1 + \delta) Q_\Phi(s,a) &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + (1 + \delta) Q_\Phi(s,a)/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1')$$
and leave (2) and (3) unchanged.  Rewrite (4) to train $Q_\Phi(s,a)$ by minimizing a squared ``Bellman Error'' between $Q_\Phi(s,a)$ and the outcome $z$ over actions drawn from the replay buffer's stored policy.
Presumably this version does not work as well.

\bigskip
{\bf Problem 3.}
We consider replacing the policy network $\pi_\Phi$ with an advantage network $A_\Phi$ so that each node $s$ stores the $A$-values $A_\Phi(s,a)$ rather than
the policy probabilities $\pi_\Phi(s,a)$.  We now have each node $s$ also store $\hat{\mu}(s)$ which equals the average value of the simulations that go through state $s$.
We then replace (1) with
$$U(s,a) =  \left\{\begin{array}{ll}(1+\delta)(A_\Phi(s,a) + V_\Phi(s)) &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + (1+\delta)(A_\Phi(s,a) + V_\Phi(s))/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1'')$$
and leave (2) unchanged and replace (3) by

\medskip
\centerline{a replay buffer of tuples $(s_{\mathrm{root}},\pi_{s_{\mathrm{root}}},z,\hat{\mu}(s_{\mathrm{root}}),\hat{\mu}(s_{\mathrm{root}},\cdot))$ \hspace{2em}$(3')$}

\medskip
Rewrite (4) to train $A_\Phi(s,a)$ by minimizing a squared ``Bellman Error'' between $A_\Phi(s,a)$ and $\hat{A}(s,a)$ defined as follows
$$\hat{A}(s,a) =  \left\{\begin{array}{ll}A_\Phi(s,a) &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) - \hat{\mu}(s) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(5)$$

\medskip
Although a valiant attempt, this version presumably also does not work as well.
It is interesting to contemplate the magic of (1) through (4) as used in AlphaZero (in spite of the seemingly ill-formed semantics of (1)).

\bigskip
{\bf Problem 4. 25 points.}
This problem is on AlphaZero.

\medskip
{\bf Backgound.}
A version of AlphaZero can be defined as follows.

\medskip
To select a move in the game, first construct a search tree over possible moves to evaluate options.
The tree is grown by running ``simulations''. 
In descending into the tree, simulations select the move $\argmax_a U(s,a)$ where we have
$$U(s,a) =  \left\{\begin{array}{ll}\lambda_u\; \pi_\Phi(s,a) &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + \lambda_u\; \pi_\Phi(s,a)/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1)$$
where $N(s,a)$ is the number of simulations that reached state $s$ and then selected action $a$ from there.

\medskip
When the search is completed, we select an action from the root position.  For this we use a post-search stochastic policy
$$\pi_{s_{\mathrm{root}}}(a) \propto N(s_{\mathrm{root}},a)^\beta\;\;\;\;\;(2)$$
where $\beta$ is temperature hyperparameter.

\medskip
The value function $V_\Phi(s)$ and the policy $\pi_\Phi(a|s)$ are trained from a replay buffer containing triples $(s,\pi,R)$ where $s$ is a position from which a tree search was done,
$\pi$ is the root probability distribution over actions computed as above from the tree search, and $R$ is the final reward of the game in which this position occurred.
The parameters $\Phi$ are trained by SGD on the following objective defined on the replay buffer.

$$\Phi^*\; = \;\argmin_\Phi \;E_{(s,\pi,R) \sim \mathrm{Replay},\;a \sim \pi}\;\left(\begin{array}{l} (V_\Phi(s) - R)^2 \\ \\ - \lambda_\pi\;\log \pi_\Phi(a|s) \\ \\ + \lambda_R\;||\Phi||^2\end{array}\right)\;\;\;\;(2)$$
  
(a) 20point. Rewrite (1) and (2) above to replace the policy $\pi_\Phi(a|s)$ with an advantage function $A_\Phi(s,a)$ so that the expected value of an action $a$ in state $s$ (the $Q$-value)
  is modeled by $V_\Phi(s) + A(s,a)$.

  \solution{
    $$U(s,a) =  \left\{\begin{array}{ll}\lambda_u\; (V_\Phi(s) + A_\Phi(s,a)) &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + \lambda_u\; (V_\Phi(s) + A_\Phi(s,a))/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1)$$
    or

    $$U(s,a) =  \left\{\begin{array}{ll}V_\Phi(s) + A_\Phi(s,a) + \lambda_u &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + (V_\Phi(s) + A_\Phi(s,a) + \lambda_u)/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1)$$

$$\Phi^*\; = \;\argmin_\Phi \;E_{(s,\pi,R) \sim \mathrm{Replay},\;a \sim \pi}\;\left(\begin{array}{l} (V_\Phi(s) - R)^2 \\ \\ \lambda_A\;(A_\Phi(s,a) - (R - V_\Phi(s))^2 \\ \\ + \lambda_R\;||\Phi||^2\end{array}\right)\;\;\;\;(2)$$
  }

  \medskip
  (b) 5 points: Will the training procedure you defined in (a) effectively train $A_\Phi(s,a)$ for off-policy actions $a$?  If not, is that a problem and does this problem arise
  in the original version in terms of $\pi_\Phi(a|s)$?  Explain your answer.

  \solution{The solution to (a) does not train $A_\Phi(s,a)$ on off-policy actions.  However, the original formulation in terms of the policy $\pi_\Phi(a|s)$ drives down the probability of
    off-policy actions so as to place large probability on the on-policy actions. This would seem to be more effective.

    \medskip
    But any answer gets full credit for thinking about it.}

  
\bigskip
{\bf Problem 4. 25 points.}
This problem is on AlphaZero.

\medskip
{\bf Backgound.}
A version of AlphaZero can be defined as follows.

\medskip
To select a move in the game, first construct a search tree over possible moves to evaluate options.
The tree is grown by running ``simulations''. 
In descending into the tree, simulations select the move $\argmax_a U(s,a)$ where we have
$$U(s,a) =  \left\{\begin{array}{ll}\lambda_u\; \pi_\Phi(s,a) &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + \lambda_u\; \pi_\Phi(s,a)/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1)$$
where $N(s,a)$ is the number of simulations that reached state $s$ and then selected action $a$ from there.

\medskip
When the search is completed, we select an action from the root position.  For this we use a post-search stochastic policy
$$\pi_{s_{\mathrm{root}}}(a) \propto N(s_{\mathrm{root}},a)^\beta\;\;\;\;\;(2)$$
where $\beta$ is temperature hyperparameter.

\medskip
The value function $V_\Phi(s)$ and the policy $\pi_\Phi(a|s)$ are trained from a replay buffer containing triples $(s,\pi,R)$ where $s$ is a position from which a tree search was done,
$\pi$ is the root probability distribution over actions computed as above from the tree search, and $R$ is the final reward of the game in which this position occurred.
The parameters $\Phi$ are trained by SGD on the following objective defined on the replay buffer.

$$\Phi^*\; = \;\argmin_\Phi \;E_{(s,\pi,R) \sim \mathrm{Replay},\;a \sim \pi}\;\left(\begin{array}{l} (V_\Phi(s) - R)^2 \\ \\ - \lambda_\pi\;\log \pi_\Phi(a|s) \\ \\ + \lambda_R\;||\Phi||^2\end{array}\right)\;\;\;\;(2)$$
  
(a) 20point. Rewrite (1) and (2) above to replace the policy $\pi_\Phi(a|s)$ with an advantage function $A_\Phi(s,a)$ so that the expected value of an action $a$ in state $s$ (the $Q$-value)
  is modeled by $V_\Phi(s) + A(s,a)$.

  \solution{
    $$U(s,a) =  \left\{\begin{array}{ll}\lambda_u\; (V_\Phi(s) + A_\Phi(s,a)) &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + \lambda_u\; (V_\Phi(s) + A_\Phi(s,a))/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1)$$
    or

    $$U(s,a) =  \left\{\begin{array}{ll}V_\Phi(s) + A_\Phi(s,a) + \lambda_u &\mbox{if $N(s,a) = 0$} \\ \hat{\mu}(s,a) + (V_\Phi(s) + A_\Phi(s,a) + \lambda_u)/N(s,a) & \mbox{otherwise} \end{array}\right.\;\;\;\;\;(1)$$

$$\Phi^*\; = \;\argmin_\Phi \;E_{(s,\pi,R) \sim \mathrm{Replay},\;a \sim \pi}\;\left(\begin{array}{l} (V_\Phi(s) - R)^2 \\ \\ \lambda_A\;(A_\Phi(s,a) - (R - V_\Phi(s))^2 \\ \\ + \lambda_R\;||\Phi||^2\end{array}\right)\;\;\;\;(2)$$
  }

  \medskip
  (b) 5 points: Will the training procedure you defined in (a) effectively train $A_\Phi(s,a)$ for off-policy actions $a$?  If not, is that a problem and does this problem arise
  in the original version in terms of $\pi_\Phi(a|s)$?  Explain your answer.

  \solution{The solution to (a) does not train $A_\Phi(s,a)$ on off-policy actions.  However, the original formulation in terms of the policy $\pi_\Phi(a|s)$ drives down the probability of
    off-policy actions so as to place large probability on the on-policy actions. This would seem to be more effective.

    \medskip
    But any answer gets full credit for thinking about it.}

\end{document}

