\input ../../SlidePreamble
\input ../../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \vfill
  \centerline{\bf Deep Graphical Models}
  \vfill
  \centerline{\bf aka, Energy Based Models}
\vfill
\vfill
\vfill

\slide{Distributions on Exponentially Large Sets}

\vfill
{\color{red}
$$\Phi^* = \argmin_\Phi E_{(x,y) \sim \mathrm{Pop}}\;-\ln \;P(y|x)$$

\vfill
$$\Phi^* = \argmin_\Phi E_{y \sim \mathrm{Pop}}\;-\ln \;P(y)$$
}

{\color{red} The structured case:} $y \in {\cal Y}$ where ${\cal Y}$ is discrete but {\color{red} iteration over $\hat{y} \in {\cal Y}$ is infeasible}.
\slide{Semantic Segmentation}
\centerline{\includegraphics[height = 2.5in]{\images/SemSeg}}

\vfill
We want to assign each pixel to one of $Y$ semantic classes.

\vfill
For example ``person'', ``car'', ``building'', ``sky'' or ``other''.

\slide{Constructing a Graph}

We construct a graph whose nodes are the pixels and where there is an edges between each pixel and its four nearest neighboring pixels.

\vfill
\centerline{\includegraphics[height= 4in]{\images/grid}}

\slide{Labeling the Nodes of a Graph}
\centerline{\includegraphics[height= 1.5in]{\images/Graph}}     
\medskip

$\hat{y} $ assigns a semantic class $\hat{y}[n]$ to each node (pixel) $n$.

\vfill
We assign a score to $\hat{y}$ by assigning a score to each node and each edge of the graph.

{\color{red} $$s(\hat{y}) = \sum_{n \in \mathrm{Nodes}}\; s^N[n,\hat{y}[n]]\; + \sum_{\tuple{n,m} \in \mathrm{Edges}}\;s^E[\tuple{n,m},\hat{y}[n],\hat{y}[m]]$$}
\centerline{Node Scores \hspace{6em}Edge Scores \hspace{3em}~}

\slide{Using Deep Networks}

For input $x$ we use a network to compute the score tensors.

\vfill
\begin{eqnarray*}
s^N[N,Y] & = & f^N_\Phi(x) \\
\\
\\
s^E[E,Y,Y] & = & f^E_\Phi(x)
\end{eqnarray*}

\slide{Exponential Softmax}

{\huge
$$\begin{array}{lrcl}
\mbox{for}\;\hat{y} & {\color{red} s(\hat{y})} & = & \sum_n\;s^N[n,\hat{y}[n]] + \sum_{\tuple{n,m} \in \mathrm{Edges}}\;s^E[\tuple{n,m},\hat{y}[n],\;\hat{y}[m]] \\
\\
\\
\mbox{for}\;\hat{y} & {\color{red} P_s(\hat{y})} & = & \softmax_{\hat{y}}\;s(\hat{y}) \;\;\;\mbox{\color{red} all possible $\hat{y}$} \\
\\
 & {\cal L} & = & - \ln P_s(y) \;\;\;{\color{red} \;\;\;\;\;\;\mbox{gold label (training label) $y$}}
\end{array}$$
}

\slide{Exponential Softmax is Typically Intractable}
\centerline{\includegraphics[height= 1.5in]{\images/Graph}}
\medskip
$\hat{y} $ assigns a label $\hat{y}[n]$ to each node $n$.

\vfill
$s(\hat{y})$ is defined by a sum over node and edge tensor scores.

\vfill
$P_s(\hat{y})$ is defined by an exponential softmax over $s(\hat{y})$.

\vfill
Computing $Z$ in general is \#P hard (there is an easy direct reduction from SAT).

\slidetwo{Compactly Representing Scores}{on Exponentially Many Labels}

The tensor {\color{red} $s^N[N,Y]$} holds $NY$ scores.

\vfill
The tensor {\color{red} $s^E[E,Y,Y]$} holds $EY^2$ scores where $e$ ranges over edges $\tuple{n,m} \in \mathrm{Edges}$.

\slide{Back-Propagation Through Exponential Softmax}

\begin{eqnarray*}
s^N[I,Y] & = & f^N_\Phi(x) \\
s^E[E,Y,Y] & = & f^E_\Phi(x)
\end{eqnarray*}

\vfill
\begin{eqnarray*}
{\color{red} s(\hat{y})} & = & \sum_n\;s^N[n,\hat{y}[n]] + \sum_{\tuple{n,m} \in \mathrm{Edges}}\;s^E[\tuple{n,m},\hat{y}[n],\;\hat{y}[m]] \\
\\
{\color{red} P_s(\hat{y})} & = & \softmax_{\hat{y}}\;s(\hat{y}) \;\;\mbox{\color{red} all possible $\hat{y}$} \\
\\
{\cal L} & = & {\color{red} - \ln P_s(y) \;\;\;\mbox{gold label $y$}}
\end{eqnarray*}

\vfill
We want the gradients {\color{red} $s^N.\grad[N,Y]$} and {\color{red} $s^E.\grad[E,Y,Y]$}.


\slide{END}

}

\end{document}
