{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import edf\n",
    "import mnist_loader\n",
    "\n",
    "train_images, train_labels = mnist_loader.load_mnist(section = 'training', path = 'MNIST')\n",
    "test_images, test_labels = mnist_loader.load_mnist(section = 'testing', path = 'MNIST')\n",
    "\n",
    "plt.imshow(train_images[0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for debuggin purposes it is useful to\n",
    "make the computation deterministic\"\"\"\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Now we build a model with random parameters\n",
    "\n",
    "x = edf.Input((28*28,))\n",
    "y = edf.Input((1,))\n",
    "    \n",
    "W1 = edf.Parameter(np.random.randn(20,28*28)*.1)\n",
    "W2 = edf.Parameter(np.random.randn(10,20)*.1)\n",
    "    \n",
    "L1 = edf.Sigmoid(edf.Norm(edf.VDot(W1,x)))\n",
    "L2 = edf.Softmax(edf.Norm(edf.VDot(W2,L1)))\n",
    "M = edf.Model(edf.NegLog(edf.Aref(L2,y)))\n",
    "    \n",
    "\"\"\"the following is needed by the learning code below to set\n",
    "the inputs (input and label are both \"inputs\" to the model)\"\"\"\n",
    "    \n",
    "M.x = x\n",
    "M.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"the verify procedure does backpropagation and numerical\n",
    "differentiation and compares to the two. This is important\n",
    "for debugging componets.  When writing a new component\n",
    "one should verify a stub model that uses just that component.\n",
    "Discrepencies between backprop and numerical differntiation\n",
    "can be due to bugs, numerical precision issues, or nonlinearities\n",
    "in the model.  It is good to have your stub model is\n",
    "operating in a numerically stable regime.  Beware of sigmoids causing\n",
    "vanishing gradients.\"\"\"\n",
    "\n",
    "x.value = np.random.rand(28*28)\n",
    "y.value = np.zeros(1,np.int64)\n",
    "\n",
    "edf.verify(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the following functions are used to train and evaluate the\n",
    "network. training is done by iterating over all training samples\n",
    "and updating the model's parameters with gradient descent.\"\"\"\n",
    "\n",
    "def train(M, nepochs):\n",
    "    train_loss_curve = []\n",
    "    train_err_curve = []\n",
    "    test_loss_curve = []\n",
    "    test_err_curve = []\n",
    "    for epoch in range(nepochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, nepochs))\n",
    "        train_an_epoch(M)\n",
    "        train_loss, train_err = evaluate(M, train_images, train_labels)\n",
    "        train_loss_curve.append(train_loss)\n",
    "        train_err_curve.append(100*train_err)\n",
    "        print (\"\\t Training: Loss {:.3f} Error {:.2f}\".format(train_loss, train_err))\n",
    "        test_loss, test_err = evaluate(M, test_images, test_labels)\n",
    "        test_loss_curve.append(test_loss)\n",
    "        test_err_curve.append(100*test_err)\n",
    "        print (\"\\t Test: Loss {:.3f} Error {:.2f}\".format(test_loss, test_err))\n",
    "    return train_loss_curve, train_err_curve, test_loss_curve, test_err_curve\n",
    "\n",
    "def train_an_epoch(M):\n",
    "    numsamples = train_images.shape[0]\n",
    "    for i in range(numsamples):\n",
    "        M.x.value = train_images[i].reshape((28*28,))\n",
    "        M.y.value = [train_labels[i]]\n",
    "        M.backprop()\n",
    "        if i%20000 == 0:\n",
    "            print (\"\\t Batch {}/{}\".format(i, numsamples))\n",
    "\n",
    "def evaluate(M, data, labels):\n",
    "    total_loss = 0.0\n",
    "    total_mistakes = 0.0\n",
    "    numsamples = data.shape[0]\n",
    "    for i in range(numsamples):\n",
    "        M.x.value = data[i].reshape((28*28,))\n",
    "        M.y.value = [labels[i]]\n",
    "        M.forward()\n",
    "        total_loss = total_loss + np.mean(M.loss.value)\n",
    "        total_mistakes = total_mistakes + int(np.argmax(L2.value) != labels[i])\n",
    "    return total_loss/numsamples, total_mistakes/numsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now, we are ready to train a network on MNIST. the following will\n",
    "iterate over the training dataset a total of 10 times (10 epochs).\"\"\"\n",
    "\n",
    "train_loss_curve, train_err_curve, test_loss_curve, test_err_curve = train(M,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"we can then plot the loss and error curves on the training and test data.\"\"\"\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(len(test_loss_curve)), test_loss_curve, color='red')\n",
    "plt.plot(np.arange(len(train_loss_curve)), train_loss_curve, color='blue')\n",
    "plt.legend(['test loss', 'train loss'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(np.arange(len(test_err_curve)), test_err_curve, color='red')\n",
    "plt.plot(np.arange(len(train_err_curve)), train_err_curve, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"next, you have to implement a ReLU activation function ReLU(x) = max(0,x).\n",
    "implement the forward and backward methods of the following class. use the\n",
    "provided EDF source code to understand what each method should do. note that\n",
    "since ReLU has no parameters, it might be helpful to base your implementation\n",
    "on how the Sigmoid EDF Component is implemented.\"\"\"\n",
    "\n",
    "class ReLU(edf.Component):\n",
    "    \"\"\"for y = ReLU(x) we have that y has the same shape\n",
    "    as x where y[i1,...,ik] = relu(x[i1,...ik]),\n",
    "    where relu(z) = max(0,z)\"\"\"\n",
    "    def __init__(self,x):\n",
    "        self.value = np.empty(x.shape,np.float32)\n",
    "        self.x = x\n",
    "        edf.Component.__init__(self)\n",
    "        edf.input_of(x,self)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the code below will plot the output and gradients computed by your\n",
    "implementation of the ReLU component above. check if the plots match\n",
    "ReLU(x) and dReLU/dx(x) as a sanity test of your implementation.\"\"\"\n",
    "\n",
    "x = edf.Input((1,))\n",
    "y = edf.Input((1,))\n",
    "relu = ReLU(x)\n",
    "M = edf.Model(relu)\n",
    "M.x = x\n",
    "M.y = y\n",
    "\n",
    "outputs = []\n",
    "grads = []\n",
    "values = np.linspace(-2,2,100)\n",
    "\n",
    "for v in values:\n",
    "    M.x.value = [v]\n",
    "    M.y.value = [1]\n",
    "    M.backprop()\n",
    "    outputs.append(relu.value[0])\n",
    "    grads.append(relu.x.grad[0])\n",
    "\n",
    "plt.xlabel(\"value\")\n",
    "plt.plot(values, outputs, color='red')\n",
    "plt.plot(values, grads, color='blue')\n",
    "plt.legend(['output', 'grad'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now, train the same network as before but with a ReLU activation\n",
    "in the hidden layer instead of a Sigmoid.\"\"\"\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "x = edf.Input((28*28,))\n",
    "y = edf.Input((1,))\n",
    "    \n",
    "W1 = edf.Parameter(np.random.randn(20,28*28)*.1)\n",
    "W2 = edf.Parameter(np.random.randn(10,20)*.1)\n",
    "    \n",
    "L1 = ReLU(edf.Norm(edf.VDot(W1,x)))\n",
    "L2 = edf.Softmax(edf.Norm(edf.VDot(W2,L1)))\n",
    "M = edf.Model(edf.NegLog(edf.Aref(L2,y)))\n",
    "  \n",
    "M.x = x\n",
    "M.y = y\n",
    "\n",
    "x.value = np.random.rand(28*28)\n",
    "y.value = np.zeros(1,np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_curve, train_err_curve, test_loss_curve, test_err_curve = train(M,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(len(test_loss_curve)), test_loss_curve, color='red')\n",
    "plt.plot(np.arange(len(train_loss_curve)), train_loss_curve, color='blue')\n",
    "plt.legend(['test loss', 'train loss'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(np.arange(len(test_err_curve)), test_err_curve, color='red')\n",
    "plt.plot(np.arange(len(train_err_curve)), train_err_curve, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"finally, implement the Tanh activation function by filling the missing\n",
    "code in the forward and backward methods below. it might be helpful to derive\n",
    "a relationship between Tanh and Sigmoid so that you can re-use parts of \n",
    "EDF's Sigmoid code.\"\"\"\n",
    "\n",
    "class Tanh(edf.Component):\n",
    "    \"\"\"for y = ReLU(x) we have that y has the same shape\n",
    "    as x where y[i1,...,ik] = tanh(x[i1,...ik]),\n",
    "    where tanh(z) = (e^z - e^-z)/(e^z + e^-z)\"\"\"\n",
    "    def __init__(self,x):\n",
    "        self.value = np.empty(x.shape,np.float32)\n",
    "        self.x = x\n",
    "        edf.Component.__init__(self)\n",
    "        edf.input_of(x,self)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"again, make sure that the output and gradients plotted below are correct.\"\"\"\n",
    "\n",
    "x = edf.Input((1,))\n",
    "y = edf.Input((1,))\n",
    "tanh = Tanh(x)\n",
    "M = edf.Model(tanh)\n",
    "M.x = x\n",
    "M.y = y\n",
    "\n",
    "outputs = []\n",
    "grads = []\n",
    "values = np.linspace(-5,5,100)\n",
    "\n",
    "for v in values:\n",
    "    M.x.value = [v]\n",
    "    M.y.value = [1]\n",
    "    M.backprop()\n",
    "    outputs.append(tanh.value[0])\n",
    "    grads.append(tanh.x.grad[0])\n",
    "\n",
    "plt.xlabel(\"value\")\n",
    "plt.plot(values, outputs, color='red')\n",
    "plt.plot(values, grads, color='blue')\n",
    "plt.legend(['output', 'grad'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"you can then train a network with a Tanh activation function instead\n",
    "of Sigmoid/ReLU.\"\"\"\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "x = edf.Input((28*28,))\n",
    "y = edf.Input((1,))\n",
    "    \n",
    "W1 = edf.Parameter(np.random.randn(20,28*28)*.1)\n",
    "W2 = edf.Parameter(np.random.randn(10,20)*.1)\n",
    "    \n",
    "L1 = Tanh(edf.Norm(edf.VDot(W1,x)))\n",
    "L2 = edf.Softmax(edf.Norm(edf.VDot(W2,L1)))\n",
    "M = edf.Model(edf.NegLog(edf.Aref(L2,y)))\n",
    "  \n",
    "M.x = x\n",
    "M.y = y\n",
    "\n",
    "x.value = np.random.rand(28*28)\n",
    "y.value = np.zeros(1,np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_curve, train_err_curve, test_loss_curve, test_err_curve = train(M,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(len(test_loss_curve)), test_loss_curve, color='red')\n",
    "plt.plot(np.arange(len(train_loss_curve)), train_loss_curve, color='blue')\n",
    "plt.legend(['test loss', 'train loss'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(np.arange(len(test_err_curve)), test_err_curve, color='red')\n",
    "plt.plot(np.arange(len(train_err_curve)), train_err_curve, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
