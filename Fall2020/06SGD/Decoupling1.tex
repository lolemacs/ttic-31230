\input ../../SlidePreamble
\input ../../preamble

\newcommand{\solution}[1]{\bigskip {\bf Solution}: #1}

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{\bf Stochastic Gradient Descent (SGD)}
  \vfill
  \centerline{\bf Decoupling the Learning Rate From the Batch Size}
  
\slide{Decoupling $\eta$ from $B$}

For vanilla SGD with minibatching we have

\begin{eqnarray*}
\Phi_{t+1} & \;\minuseq\; & \eta \hat{g}_t \\
\\
\hat{g}_t & = & \frac{1}{B} \sum_b \hat{g}_{t,b}
\end{eqnarray*}

\vfill
Where $\hat{g}_{t,b}$ is the gradient of the element $b$ of the batch.

\slide{Decoupling $\eta$ from $B$}

For batch size 1 on the same sequence of data points with $b \in \{1,\ldots,B\}$ and with learning rate $\eta_0$ we have

\begin{eqnarray*}
\Phi_{t+B} & = &  \Phi_{t} - \sum_b \;\eta_0\;\nabla_\Phi {\cal L}(b,\Phi_{t+b-1}) \\
\\
& \approx & \eta_0 \sum_b \nabla_\Phi {\cal L}(b,\Phi_t) \\
\\
& = & B\eta_0\; \hat{g}_t
\end{eqnarray*}

If $\eta_0$ is the optimal learning rate for $B = 1$ then $B\eta_0$ should be the optimal learning rate for general $B$.

\slide{Decoupling $\eta$ from $B$}

Recent work has show that using $\eta = B\eta_0$ leads to effective learning with very large (highly parallel)
batches.

\vfill
{\bf Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}, Goyal et al., 2017.


\slide{END}

} \end{document}

