\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning, winter 2020}
\medskip
\centerline{\bf Quiz 3}


\medskip
{\bf PAC-Bayes Background for the problem 1.} Consider any probability distribution $P(h)$ over a discrete class ${\cal H}$.
Assume $0 \leq {\cal L}(h,x,y) \leq \lmax$. Define
\begin{eqnarray*}
{\cal L}(h)  & = &  E_{(x,y)\sim \mathrm{Pop}}\;{\cal L}(h,x,y) \\
\\
\hat{{\cal L}}(h) & = & E_{(x,y)\sim \mathrm{Train}}\;{\cal L}(h,x,y)
\end{eqnarray*}
We now have the theorem that with probability
at least $1-\delta$ over the draw of training data the following holds simultaneously for all $h$.
$${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{\ln \frac{1}{P(h)} + \ln\frac{1}{\delta}}} \;\;\;(1)$$

This motivates
$$h^* = \argmin_h \;\hat{{\cal L}}(h) + \frac{5\lmax}{N} \ln\frac{1}{P(h)}\;\;\;\;(2)$$
The Bayesian maximum a-posteriori (MAP) rule is
$$h^* = \argmax_h\;P(h)\prod_{(x,y) \in \mathrm{Train}}\;P(y|x,h)\;\;\;\;(3)$$
{\bf Problem 1. Finite Precision Parameters. (25 points)}

\medskip
(a) Consider a model where the parameter vector $\Phi$ has $d$ parameters each of which is represented by a 16 bit floating point number.
Express the bound (1) in terms of the dimension $d$ assuming all parameter vectors are equally likely.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{16d\ln 2 + \ln\frac{1}{\delta}}}$$
}

\medskip
(b) Assume a variable precision representation of numbers where $\Phi[i]$ is given with $|\Phi[i]|$ bits.  Express the bound (1) as a function of $\Phi$ assuming that $P(\Phi)$
is defined so that each parameter is selected independently and that
$$P(\Phi[i]) = 2^{-|\Phi[i]|}$$.

\solution{
  \begin{eqnarray*}
    {\cal L}(h) & \leq & \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{|\Phi|\ln 2 + \ln\frac{1}{\delta}}} \\
    \\
    |\Phi| & = & \sum_i |\Phi[i]|
  \end{eqnarray*}
}

\medskip
(c) Repeat part (a) but for a model with $d$ parameters represented by
$\Phi_i = z[J[i]]$ where $J[i]$ is an integer index with $0 \leq J[i] < k$ and where $z[j]$ is a $b$ bit floating point number and where all parameter vectors are equally likely.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{kb\ln 2 + d\ln k + \ln\frac{1}{\delta}}}$$
  Since $d$ is large this is typically much tighter bound than using floating point or even integer representationfs of parameters.  It is a much more compact representaiton
  of the parameters.
}

\bigskip
~{\bf Problem 2. Pseudolikelihood of a one dimensional spin glass. (25 points)}
We let $\hat{x}$ be an assignment of a value to every node where the nodes are numbered from $1$ to $N_\mathrm{nodes}$ and for every node $i$ we have
$\hat{x}[i] \in \{0,1\}$. We define the score of $\hat{x}$ by
$$f(\hat{x}) = \sum_{i = 1}^{N-1} \bbone[\hat{x}[i] = \hat{x}[i+1]]$$
The probability distribution over assignments is defined by a softmax.  We let $\hat{x}[i:=v]$ be the assignment identical to $\hat{x}$ except that node $i$ is assigned the value $v$.
The expression $\hat{x}[i]=v$ is either true or false depending on whether no $i$ is assigned value $v$ in $\hat{x}$.  So these are quite different.
$$P_f(\hat{x}) = \softmax_{\hat{x}} f(\hat{x})$$
Pseudolikelihood is defined in terms of the softmax probability $P_f$ as follows.
\begin{eqnarray*}
  \tilde{P}_f(\hat{x}) & = & \prod_i P_f(\hat{x}[i] \;|\; \hat{x}\backslash i)
\end{eqnarray*}
What is the pseudolikelihood of the all ones assignment under the definition of $f$ given above?

\solution{
In a graphical model $P_f(\hat{x}[i] \;|\; \hat{x}/i)$ is determined by the neighbors of $i$ and we can consider only how a value is scored against it neighbors.  For $\hat{x}$ equal to all ones we have

\begin{eqnarray*}
f(\hat{x}) & = & N-1 \\
\\
f(\hat{x}[i := 0]) & = & \left\{\begin{array}{ll} N-3 & \mbox{for $1 < i < N$} \\ N-2 & \mbox{for $i=1$ or $i=N$} \end{array}\right.
\end{eqnarray*}
For $1 < i < N$ we get
\begin{eqnarray*}
Q_f(\hat{x}[i=1]\; |\;\hat{x}/i) & = & \frac{e^{N-1}}{e^{N-1} + e^{N-3}} \\
\\
& = & \frac{1}{1 + e^{-2}}
\end{eqnarray*}
and for $i = 1$ or $i = N$ we get
$$Q_f(\hat{x}[i=1]\; |\;\hat{x}/i) = \frac{1}{1 + e^{-1}}$$

This gives

$$\tilde{Q}(\hat{x}) = (1 + e^{-1})^{-2}(1 + e^{-2})^{-(N-2)}$$
}


\bigskip
~{\bf Problem 3. Pseudolikelihood for Monocular Distance Estimation. (25 points)} Here we are interested in labeling each pixed with a distance from the camera.
Each pixel $i$ is to be labeled with a real number $\hat{y}[i] > 0$ giving the distance in (say) meters from the camera to the point on the object displayed by that pixel.  We assume a neural network
that computes for each pixel $i$ an expected distance $\mu_i$ and a variance $\sigma_i > 0$.  For each pair of neighboring pixels $i$ and $j$
the neural network computes a real number $\lambda_{\tuple{i,j}} \geq 0$. For each assignment $\hat{y}$ of distances to pixels we then define the score $s(\hat{y})$ by
\begin{eqnarray*}
  s(\hat{y}) & = & \sum_{i\in \mathrm{nodes}}\;- (\hat{y}[i] - \mu_i)^2/\sigma_i^2\;\; + \sum_{\tuple{i,j} \in \mathrm{edges}}\;- \lambda_{\tuple{i,j}}|\hat{y}[i]-\hat{y}[j]|
\end{eqnarray*}

\medskip
(a) This scoring function determines a continuous softmax distribution defined by
$$p(\hat{y}) = \frac{1}{Z}e^{s(\hat{y})}$$
where $Z$ is an integral rather than a sum.  What is the dimension of the space to be integrated over in computing $Z$?

\solution{
 This is an integration over $\mathbb{R}^N$ where $N$ is the number of nodes --- an $N_\mathrm{nodes}$ dimentional space.
}

\medskip
(b) We now consider pseudolikelihood for this problem. Give an expression for the continuous conditional probability density on $\hat{y}[i]$ for the distance $\hat{y}[i]$
conditioned on the value of the neighbors $N(i)$ of node $i$.  This probability is written $p(\hat{y}[i]\;|\;\hat{y}[N(i)])$.  You answer should be given as a function of the values
$\hat{y}[j]$ for the nodes $j$ neighboring $i$ written $j \in N(i)$.   Write $Z$ as an integral but do not bother trying to solve it.  What is the dimention of the integral for this
conditional probability?

\solution{
  \begin{eqnarray*}
    p(\hat{y}[i]\;|\;\hat{y}[N(i)]) & = & \frac{1}{Z} \exp\left(- (\hat{y}[i] - \mu_i)^2/\sigma_i^2 +  \sum_{j \in N(i)}\;- \lambda_{\tuple{i,j}}|\hat{y}[i]-\hat{y}[j]|\right) \\
    \\
    Z & = & \int_0^\infty  \exp\left(- (x - \mu_i)^2/\sigma_i^2 +  \sum_{j \in N(i)}\;- \lambda_{\tuple{i,j}}|x-\hat{y}[j]|\right) dx
  \end{eqnarray*}
  This is an integral over a one dimensional space (a single real number).
}

\bigskip
{\bf Problem 4. Generalization Bounds for the realizable case. (25 points)}

Consider a finite hyothesis class ${\cal H}$ and a population distribution $\pop$ on pairs $\tuple{x,y}$ such that for $\tuple{x,y}$ drawn from the
population and $h \in {\cal H}$ we have that $h$ makes a prediction for $y$ which we will write as $h(x)$.  The error rate of hypothesis $h$ on the population is defined by
$$\mathrm{Err}_{\pop}(h) = P_{\tuple{x,y} \sim \pop}(h(x) \not = y)$$
We draw a training sample $\mathrm{Train}$ consisting of $N_{\mathrm{Train}}$ pairs $\tuple{x,y}$ drawn IID from the population.
$$\mathrm{Err}_{\mathrm{train}}(h) = \frac{1}{N_{\mathrm{train}}} \sum_{\tuple{x,y} \in \mathrm{Train}}\;\bbone(h(x) \not = y)$$

\medskip
(a) For a given hypothesis $h$ with error rate $\epsilon$ what is the probability that $\mathrm{Err}_{\mathrm{train}}(h) = 0$.

\solution{
  $(1-\epsilon)^{N_{\mathrm{train}}}$
}

\medskip
(b) We now consider a fixed threshold $\epsilon$ and consider the hypotheses $h$ satisfying $\mathrm{Err}_{\pop} \geq \epsilon$.
We will call these the ``bad'' hypotheses. 

\medskip
The simple form of the union bound is
$$P(A \cup B) \leq P(A) + P(B)$$
This can be generalized to
$$P(\exists z\; Q(z)) \leq \sum_z P(Q(z))$$
where $Q(z)$ is any statement about $z$.  

\medskip
Use your answer to (a) and the union bound to give an upper bound on the probability that 
there exists a bad hypothesis $h$ with $\mathrm{Err}_{\mathrm{train}}(h) = 0$.
You solution should be stated in terms of $\epsilon$, the number of elements $|{\cal H}|$ of ${\cal H}$,
and the number of training pairs $N_{\mathrm{train}}$.
Simplify your solution using the inequality $1-\epsilon \leq e^{-\epsilon}$.

\solution{
  \begin{eqnarray*}
    & & P(\exists h\; \;\mathrm{Err}_{\pop} \geq \epsilon,\; \mathrm{Err}_{\mathrm{train}}(h) = 0) \\
    \\
    & \leq & \sum_{h:\;\mathrm{Err}_{\pop}(h) \geq \epsilon}\;P(\mathrm{Err}_{\mathrm{train}}(h) = 0) \\
    \\
    & \leq & |{\cal H}| (1-\epsilon)^{N_{\mathrm{train}}} \\
    \\
    & \leq & |{\cal H}| e^{-N_{\mathrm{train}}\epsilon} \\
  \end{eqnarray*}
}

\medskip
(c) Now consider a small positive number $\delta$ and solve for $\epsilon$ such that the probability that a bad hypothesis has zero training error is less than $\delta$.
Your solution gives a value of $\epsilon$  such that with probability $1-\delta$ over the draw of the training error all hypothesis with zero training error
have population error no lareger than $\epsilon$.

\solution{
  \begin{eqnarray*}
    \delta & = & |{\cal H}| e^{-N_{\mathrm{train}}\epsilon} \\
    \\
    \epsilon & = & \frac{\ln |{\cal H}| + \ln \frac{1}{\delta}}{N_{\mathrm{train}}}
  \end{eqnarray*}
  }

\end{document}
