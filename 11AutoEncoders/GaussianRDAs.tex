\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}
  \vfill
  \centerline{Gaussian Noisy Channel RDAs}
  \vfill
  \vfill
  
\slide{Gaussian Noisy-Channel RDA} 
$$\Phi^* = \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}))$$

{\color{red}
\begin{eqnarray*}
\tilde{z}[i] & = & z_\Phi(y)[i] + \sigma_\Phi(y)\epsilon[i]\;\;\;\epsilon[i] \sim {\cal N}(0,1) \\
\\
p_\Phi(\tilde{z}[i]|y) & = & {\cal N}(z_\Phi(y)[i],\sigma_\Phi(y)[i]) \\
\\
q_\Phi(\tilde{z}[i]) & = & {\cal N}(\mu_q[i],\sigma_q[i]) \\
\\
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2
\end{eqnarray*}
}

\slide{Gaussian Noisy-Channel RDA}

$$\Phi^* = \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}))$$

\vfill
We will show that in the Gaussian case can fix $q_\Phi$

{\color{red}
\begin{eqnarray*}
p_\Phi(\tilde{z}[i]|y) & = & {\cal N}(z_\Phi(y)[i],\sigma_\Phi(y)[i]) \\
\\
q_\Phi(\tilde{z}[i]) & = & {\cal N}(0,1) \\
\\
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2
\end{eqnarray*}
}


\slide{Gaussian Noisy-Channel RDA}

\begin{eqnarray*}
\Phi^* &  = & \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z})) \\
\\
\\
& = & \argmin_{\Phi}\;E_{y\sim \pop} \left(\begin{array}{l}\;\;\;\;KL(p_\Phi(\tilde{z}|y),q_\Phi(\tilde{z})) \\
\\
+ \lambda \; E_\epsilon\;\mathrm{Dist}(y,\;y_\Phi(\tilde{z}))\end{array}\right)
\end{eqnarray*}

\slide{Closed Form KL-Divergence}

\begin{eqnarray*}
& & KL(p_\Phi(\tilde{z}|y),q_\Phi(\tilde{z})) \\
\\
\\
& = & \sum_i \;\frac{\sigma_\Phi(y)[i]^2 + (z_\Phi(y)[i]-\mu_q[i])^2}{2 \sigma_q[i]^2}
+ \ln\frac{\sigma_q[i]}{\sigma_\Phi(y)[i]} - \frac{1}{2}
\end{eqnarray*}


\slide{Standardizing $p_\Phi(z)$}

\begin{eqnarray*}
 &  & KL(p_\Phi(\tilde{z}|y),p_\Phi(\tilde{z})) \\
 \\
 & = & \sum_i \;\frac{ \sigma_\Phi(y)[i]^2 +(z_\Phi(y)[i] - \mu_q[i])^2}{2\sigma_q[i]^2}
+ \ln\frac{\sigma_q[i]}{\sigma_\Phi(y)[i]}
- \frac{1}{2}
\\
\\
\\
\\
 &  & KL(p_{\Phi'}(\tilde{z}|y),{\cal N}(0,I)) \\
 \\
 & = & \sum_i \;\frac{\sigma^\epsilon_{\Phi'}(y)[i]^2 +z_{\Phi'}(y)[i]^2}{2} + \ln\frac{1}{\sigma^\epsilon_{\Phi'}(y)[i]} - \frac{1}{2}
\end{eqnarray*}

\slide{Standardizing $p_\Phi(z)$}

\begin{eqnarray*}
KL_\Phi & = & \sum_i \;\frac{ \sigma_\Phi(y)[i]^2 +(z_\Phi(y)[i] - \mu_q[i])^2}{2\sigma_q[i]^2}
+ \ln\frac{\sigma_q[i]}{\sigma_\Phi(y)[i]} - \frac{1}{2}
\\
KL_{\Phi'} & = & \sum_i \;\frac{\sigma^\epsilon_{\Phi'}(y)[i]^2 +z_{\Phi'}(y)[i]^2}{2} + \ln\frac{1}{\sigma^\epsilon_{\Phi'}(y)[i]} - \frac{1}{2}
\end{eqnarray*}

Setting $\Phi'$ so that
\begin{eqnarray*}
z_{\Phi'}(y)[i] & = & (z_\Phi(y)[i] - \mu_q[i])/\sigma_q[i] \\
\sigma^\epsilon_{\Phi'}(y)[i] & = & \sigma_\Phi(y)[i]/\sigma_q[i]
\end{eqnarray*}

\vfill
gives {\color{red} $KL(p_{\Phi}(\tilde{z}|y),p_\Phi(\tilde{z})) = KL(p_{\Phi'}(\tilde{z}|y),{\cal N}(0,I))$}.

\slide{Sampling}

\centerline{Sample {\color{red} $\tilde{z} \sim {\cal N}(0,I)$} and compute {\color{red} $y_\Phi(\tilde{z})$}}
\vfill
\centerline{\includegraphics[width = 4in]{../images/VariationalFaces}}
\centerline{[Alec Radford]}

\slide{Summary: Rate-Distortion}

RDA: $y$ continuous, $\tilde{z}$ a bit string,

{\color{red}
\begin{eqnarray*}
\Phi^* &  = &  \argmin_\Phi E_{y \sim \pop} \;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))
\end{eqnarray*}
}

\vfill
Gaussian RDA: {\color{red} $\tilde{z} = z_\Phi(y) + \sigma_\Phi(y) \odot \epsilon$,\hspace{2em} $\epsilon \sim {\cal N}(0,I)$}

{\color{red}
\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi E_{y \sim \pop}\left(\begin{array}{l}\;\;\; \;KL(p_\Phi(\tilde{z}|y),{\cal N}(0,I)) \\ \\ + \;\;\;\lambda E_\epsilon\; \mathrm{Dist}(y,y_\Phi(\tilde{z})) \end{array}\right)
\end{eqnarray*}
}

\slide{END}

}
\end{document}
