\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}
  \vfill
  \centerline{Rate-Distortion Autoencoders (RDAs)}
  \vfill
  \centerline{Noisy Channel RDAs}
  \vfill
  \centerline{Gaussian Noisy Channel RDAs}

\slidetwo{Rate-Distortion Autoencoders}{(Image Compression)}

We compress a continuous signal $y$ to a bit string $\tilde{z}_\Phi(y)$.

\vfill
We decompress $\tilde{z}_\Phi(y)$ to $y_\Phi(\tilde{z}_\Phi(y))$.

\vfill
We can then define a rate-distortion loss.

{\color{red} $${\cal L}(\Phi) = E_{y \sim \mathrm{Pop}}\;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$}

\vfill
where $|\tilde{z}|$ is the number of bits in the bit string $\tilde{z}$.

\slide{Common Distortion Functions}

$$\Phi^* = \argmin_\Phi\;E_{y \sim \mathrm{Pop}}\;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$

\vfill
It is common to take

$$\mathrm{Dist}(y,\hat{y}) = ||y-\hat{y}||^2 \hspace{4em}(L_2)$$

\vfill
or

$$\mathrm{Dist}(y,\hat{y}) = ||y-\hat{y}||_1 \hspace{4em} (L_1)$$

\slide{CNN-based Image Compression}

These slides are loosely based on

\vfill
End-to-End Optimized Image Compression, Balle, Laparra, Simoncelli, ICLR 2017.


\vfill
\centerline{$y$\includegraphics[width=4in]{\images/deconvleft} $\;\tilde{z}\;$ \includegraphics[width=4in]{\images/deconvright}$\hat{y}$}


\slide{Rounding a Tensor}

Take $z_\Phi(y)$ can be a layer in a CNN applied to image $y$.  $z_\Phi(y)$ can have with both spatial and feature dimensions.

\vfill
Take $\tilde{z}_\Phi(y)$ to be the result of rounding each component of the continuous tensor $z_\Phi(y)$ to the nearest integer.

\vfill
$$\tilde{z}_\Phi(y)[x,y,i] = \lfloor z_\Phi(y)[x,y,i] + 1/2 \rfloor$$

\slide{Increasing Spatial Dimension in Decoding}

\centerline{$y$\includegraphics[width=4in]{\images/deconvleft} $\;\tilde{z}\;$ \includegraphics[width=4in]{\images/deconvright}$\hat{y}$}

\slide{Increasing Spatial Dimension in Decoding (Deconvolution)}

\vfill
To increase spatial dimension we use 4 times the desired output the features.

\begin{eqnarray*}
  L'_{\ell+1}[x,y,i] & = & \sigma\left(W[\Delta X, \Delta Y, J,i]\; L'_\ell[x + \Delta X, y + \Delta Y, J]\right)
\end{eqnarray*}

\vfill
We then reshape $L'_{\ell+1}[X,Y,I]$ to $L'_{\ell+1}[2X,2Y,I/4]$.

\slide{Rounding is not Differentiable}

$$\Phi^* = \argmin_\Phi \;E_{y \sim \pop}\;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$

\vfill
Because of rounding, $\tilde{z}_\Phi(y)$ is discrete and the gradients are zero.

\vfill
We will train using a differentiable approximation.

\slide{Rate: Replacing Code Length with Differential Entropy}

\begin{eqnarray*}
{\cal L}_{\mathrm{rate}}(\Phi) & = & E_{y \sim \pop}\;|\tilde{z}_\Phi(y)|
\end{eqnarray*}

\vfill
Recall that {\color{red} $\tilde{z}_\Phi(y)$} is a rounding of a continuous encoding {\color{red} $z_\Phi(y)$}.

\vfill
By using a nontrivial code for integers --- say Huffman coding integers ---
we can approximate the code length of the rounded integer with a continuous probability density.

\vfill
{\color{red} $$|\tilde{z}_\Phi(y)| \approx \sum_{x,y,i} -\ln p_\Phi(z_\Phi(y)[x,y,i])$$}

\slide{Distortion: Replacing Rounding with Noise}

We can make distortion differentiable by modeling rounding as the addition of noise.

\begin{eqnarray*}
{\cal L}_{\mathrm{dist}}(\Phi) & = & E_{y \sim \mathrm{Pop}} \;\mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y))) \\
\\
& \approx & E_{y,\epsilon} \;\mathrm{Dist}(y,\;y_\Phi(z_\Phi(y) + \epsilon))
\end{eqnarray*}

\vfill
Here $\epsilon$ is a noise vector each component of which is drawn uniformly from $(-1/2,1/2)$.

\slide{Rate: Differential Entropy vs. Discrete Entropy}

\bigskip
\centerline{\includegraphics[height=3in]{../images/RateDist6}}

Each point is a rate for an image measured in both differential entropy and discrete entropy.  The size of the rate changes as we change the weight $\lambda$.

\slide{Distortion: Noise vs. Rounding}

\centerline{\includegraphics[height=3in]{../images/RateDist5}}

Each point is a distortion for an image measured in both a rounding model and a noise model.  The size of the distortion changes as we change the weight $\lambda$.

\anaslide{JPEG at 4283 bytes or .121 bits per pixel}

\bigskip
\centerline{\includegraphics[height=5in]{../images/RateDist2}}

\anaslide{JPEG 2000 at 4004 bytes or .113 bits per pixel}

\bigskip
\centerline{\includegraphics[height= 5in]{../images/RateDist3}}

\anaslide{Deep Autoencoder at 3986 bytes or .113 bits per pixel}

\bigskip
\centerline{\includegraphics[height = 5in]{../images/RateDist4}}

\slide{Noisy-Channel RDAs}

The image compression case study training was based on a differentiable loss

\vfill
$$\Phi^* = \argmin_\Phi \left(\; E_y\;- \ln p_\Phi(z_\Phi(y))\right) + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(z_\Phi(y) + \epsilon))$$

\vfill
In a rate-distortion auto-encoder we will replace the rate term with a channel capacity (rate) for a noisy channel on continuous variables.

\slide{Mutual Information as a Channel Rate}

$$\Phi^* = \argmin_\Phi \left(\; E_y\;- \ln p_\Phi(z_\Phi(y))\right) + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(z_\Phi(y) + \epsilon))$$

\vfill
is replaced by

\begin{eqnarray*}
\tilde{z} & = & z_\Phi(y) + f_\Phi(y,\epsilon) \;\;\mbox{$\epsilon$ is fixed (parameter independent) noise} \\
\\
\Phi^* & = & \argmin_\Phi \;{\color{red} I_\Phi(y,\tilde{z})} + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(\tilde{z}))
\end{eqnarray*}

\vfill
By the channel capacity theorem {\color{red} $I(y,\tilde{z})$} is the {\bf rate} of information transfer from $y$ to $\tilde{z}$.
Differential mutual infomation is more meaningful than differential cross entropy.

\anaslide{Mutual Information as a Channel Rate}

\bigskip
\bigskip
\begin{eqnarray*}
\tilde{z} & = & z_\Phi(y) + {\color{red} f_\Phi(y,\epsilon)} \;\;\mbox{$\epsilon$ is fixed (parameter independent) noise} \\
\\
\Phi^* & = & \argmin_\Phi \;I_\Phi(y,\tilde{z}) + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(\tilde{z}))
\end{eqnarray*}

\vfill
Taking the distribution on $\epsilon$ to be parameter independent is called the ``reparameterization trick'' and allows SGD.
\begin{eqnarray*}
& & \nabla_\Phi \;E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(z_\Phi(y) + f_\Phi(\epsilon))) \\
\\
& = & E_{y,\epsilon}\; \nabla_\Phi\;\mathrm{Dist}(y,y_\Phi(z_\Phi(y) + f_\Phi(\epsilon)))
\end{eqnarray*}

\anaslide{Mutual Information as a Channel Rate}

\bigskip
\bigskip
\begin{eqnarray*}
\tilde{z} & = & z_\Phi(y) + {\color{red} f_\Phi(y,\epsilon)} \;\;\mbox{$\epsilon$ is fixed (parameter independent) noise} \\
\\
\Phi^* & = & \argmin_\Phi \;I_\Phi(y,\tilde{z}) + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(\tilde{z}))
\end{eqnarray*}

\vfill
Typically $f_\Phi(y,\epsilon)$ is simple, such as $\sigma_\Phi(y)\odot \epsilon$, so that {\color{red} $p_\Phi(\tilde{z}|y)$ is easily computed.}

\slide{Mutual Information Replaces Cross Entropy}

\begin{eqnarray*}
I_\Phi(y,\tilde{z})  & = & E_{y,\epsilon}\; \ln \frac{\mathrm{pop}(y)p_\Phi(\tilde{z}|y)}{\mathrm{pop}(y)p_{\mathrm{pop},\Phi}(\tilde{z})} \\
\\
\\
& = & E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{p_{\mathrm{pop},\Phi}(\tilde{z})} \\
\\
\\
\mathrm{where}\;\;\;\;p_{\mathrm{pop},\Phi}(\tilde{z}) & = & E_{y\sim \mathrm{pop}}\;\;p_\Phi(\tilde{z}|y)
\end{eqnarray*}

\slide{A Variational Bound}

$$p_{\mathrm{pop},\Phi}(\tilde{z})  = E_{y\sim \mathrm{pop}}\;\;p_\Phi(\tilde{z}|y)$$

\vfill
We cannot compute $p_{\mathrm{pop},\Phi}(\tilde{z})$.

\vfill
Instead we will use a variational bound involving a computable model $q_\Phi(\tilde{z})$

\slide{A Variational Bound}

\begin{eqnarray*}
{\color{red} I(y,\tilde{z})}  & = & E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{p_{\mathrm{pop},\Phi}(\tilde{z})} \\
\\
& = & E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} + E_{y,\epsilon}\;\ln\frac{q_\Phi(\tilde{z})}{p_{\mathrm{pop},\Phi}(\tilde{z})} \\
\\
& = & E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} - KL(p_{\mathrm{pop},\Phi}(\tilde{z}),q_\Phi(\tilde{z})) \\
\\
& {\color{red} \leq} & {\color{red} E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})}}
\end{eqnarray*}

\slide{A Fundamental Equation for the  Continuous Case}

\begin{eqnarray*}
\tilde{z} & = & z_\Phi(y) + f_\Phi(y,\epsilon) \\
\\
\Phi^* & = & \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}))
\end{eqnarray*}

\slide{Gaussian Noisy-Channel RDA} 
$$\Phi^* = \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}))$$

{\color{red}
\begin{eqnarray*}
\tilde{z}[i] & = & z_\Phi(y)[i] + \sigma_\Phi(y)\epsilon[i]\;\;\;\epsilon[i] \sim {\cal N}(0,1) \\
\\
p_\Phi(\tilde{z}[i]|y) & = & {\cal N}(z_\Phi(y)[i],\sigma_\Phi(y)[i]) \\
\\
q_\Phi(\tilde{z}[i]) & = & {\cal N}(\mu_q[i],\sigma_q[i]) \\
\\
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2
\end{eqnarray*}
}

\slide{Gaussian Noisy-Channel RDA}

$$\Phi^* = \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}))$$

\vfill
We will show that in the Gaussian case can fix $q_\Phi$

{\color{red}
\begin{eqnarray*}
p_\Phi(\tilde{z}[i]|y) & = & {\cal N}(z_\Phi(y)[i],\sigma_\Phi(y)[i]) \\
\\
q_\Phi(\tilde{z}[i]) & = & {\cal N}(0,1) \\
\\
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2
\end{eqnarray*}
}


\slide{Gaussian Noisy-Channel RDA}

\begin{eqnarray*}
\Phi^* &  = & \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(\tilde{z}|y)}{q_\Phi(\tilde{z})} + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z})) \\
\\
\\
& = & \argmin_{\Phi}\;E_{y\sim \pop} \left(\begin{array}{l}\;\;\;\;KL(p_\Phi(\tilde{z}|y),q_\Phi(\tilde{z})) \\
\\
+ \lambda \; E_\epsilon\;\mathrm{Dist}(y,\;y_\Phi(\tilde{z}))\end{array}\right)
\end{eqnarray*}

\slide{Closed Form KL-Divergence}

\begin{eqnarray*}
& & KL(p_\Phi(\tilde{z}|y),q_\Phi(\tilde{z})) \\
\\
\\
& = & \sum_i \;\frac{\sigma_\Phi(y)[i]^2 + (z_\Phi(y)[i]-\mu_q[i])^2}{2 \sigma_q[i]^2}
+ \ln\frac{\sigma_q[i]}{\sigma_\Phi(y)[i]} - \frac{1}{2}
\end{eqnarray*}


\slide{Standardizing $p_\Phi(z)$}

\begin{eqnarray*}
 &  & KL(p_\Phi(\tilde{z}|y),p_\Phi(\tilde{z})) \\
 \\
 & = & \sum_i \;\frac{ \sigma_\Phi(y)[i]^2 +(z_\Phi(y)[i] - \mu_q[i])^2}{2\sigma_q[i]^2}
+ \ln\frac{\sigma_q[i]}{\sigma_\Phi(y)[i]}
- \frac{1}{2}
\\
\\
\\
\\
 &  & KL(p_{\Phi'}(\tilde{z}|y),{\cal N}(0,I)) \\
 \\
 & = & \sum_i \;\frac{\sigma^\epsilon_{\Phi'}(y)[i]^2 +z_{\Phi'}(y)[i]^2}{2} + \ln\frac{1}{\sigma^\epsilon_{\Phi'}(y)[i]} - \frac{1}{2}
\end{eqnarray*}

\slide{Standardizing $p_\Phi(z)$}

\begin{eqnarray*}
KL_\Phi & = & \sum_i \;\frac{ \sigma_\Phi(y)[i]^2 +(z_\Phi(y)[i] - \mu_q[i])^2}{2\sigma_q[i]^2}
+ \ln\frac{\sigma_q[i]}{\sigma_\Phi(y)[i]} - \frac{1}{2}
\\
KL_{\Phi'} & = & \sum_i \;\frac{\sigma^\epsilon_{\Phi'}(y)[i]^2 +z_{\Phi'}(y)[i]^2}{2} + \ln\frac{1}{\sigma^\epsilon_{\Phi'}(y)[i]} - \frac{1}{2}
\end{eqnarray*}

Setting $\Phi'$ so that
\begin{eqnarray*}
z_{\Phi'}(y)[i] & = & (z_\Phi(y)[i] - \mu_q[i])/\sigma_q[i] \\
\sigma^\epsilon_{\Phi'}(y)[i] & = & \sigma_\Phi(y)[i]/\sigma_q[i]
\end{eqnarray*}

\vfill
gives {\color{red} $KL(p_{\Phi}(\tilde{z}|y),p_\Phi(\tilde{z})) = KL(p_{\Phi'}(\tilde{z}|y),{\cal N}(0,I))$}.

\slide{Sampling}

\centerline{Sample {\color{red} $\tilde{z} \sim {\cal N}(0,I)$} and compute {\color{red} $y_\Phi(\tilde{z})$}}
\vfill
\centerline{\includegraphics[width = 4in]{../images/VariationalFaces}}
\centerline{[Alec Radford]}

\slide{Summary: Rate-Distortion}

RDA: $y$ continuous, $\tilde{z}$ a bit string,

{\color{red}
\begin{eqnarray*}
\Phi^* &  = &  \argmin_\Phi E_{y \sim \pop} \;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))
\end{eqnarray*}
}

\vfill
Gaussian RDA: {\color{red} $\tilde{z} = z_\Phi(y) + \sigma_\Phi(y) \odot \epsilon$,\hspace{2em} $\epsilon \sim {\cal N}(0,I)$}

{\color{red}
\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi E_{y \sim \pop}\left(\begin{array}{l}\;\;\; \;KL(p_\Phi(\tilde{z}|y),{\cal N}(0,I)) \\ \\ + \;\;\;\lambda E_\epsilon\; \mathrm{Dist}(y,y_\Phi(\tilde{z})) \end{array}\right)
\end{eqnarray*}
}

\slide{END}

}
\end{document}
