\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}
  \vfill
  \centerline{Rate-Distortion Autoencoders (RDAs)}
  \vfill
  \centerline{Noisy Channel RDAs}
  \vfill
  \centerline{Gaussian Variational Autoencoders (Gaussian VAEs)}

\slidetwo{Rate-Distortion Autoencoders}{(Image Compression)}

We compress a continuous signal $y$ to a bit string $\tilde{z}_\Phi(y)$.

\vfill
We decompress $\tilde{z}_\Phi(y)$ to $y_\Phi(\tilde{z}_\Phi(y))$.

\vfill
We can then define a rate-distortion loss.

{\color{red} $${\cal L}(\Phi) = E_{y \sim \mathrm{Pop}}\;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$}


\slide{Common Distortion Functions}

$$\Phi^* = \argmin_\Phi\;E_{y \sim \mathrm{Pop}}\;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$

\vfill
It is common to take

$$\mathrm{Dist}(y,\hat{y}) = ||y-\hat{y}||^2 \hspace{4em}(L_2)$$

\vfill
or

$$\mathrm{Dist}(y,\hat{y}) = ||y-\hat{y}||_1 \hspace{4em} (L_1)$$

\slide{CNN-based Image Compression}

These slides are loosely based on

\vfill
End-to-End Optimized Image Compression, Balle, Laparra, Simoncelli, ICLR 2017.


\vfill
\centerline{$y$\includegraphics[width=4in]{\images/deconvleft} $\;\tilde{z}\;$ \includegraphics[width=4in]{\images/deconvright}$\hat{y}$}


\slide{Rounding a Tensor}

Take $z_\Phi(y)$ can be a layer in a CNN applied to image $y$.  $z_\Phi(y)$ can have with both spatial and feature dimensions.

\vfill
Take $\tilde{z}_\Phi(y)$ to be the result of rounding each component of the continuous tensor $z_\Phi(y)$ to the nearest integer.

\vfill
$$\tilde{z}_\Phi(y)[x,y,i] = \lfloor z_\Phi(y)[x,y,i] + 1/2 \rfloor$$

\slide{Increasing Spatial Dimension in Decoding}

\centerline{$y$\includegraphics[width=4in]{\images/deconvleft} $\;\tilde{z}\;$ \includegraphics[width=4in]{\images/deconvright}$\hat{y}$}

\slide{Increasing Spatial Dimension in Decoding (Deconvolution)}

\vfill
To increase spatial dimension we use 4 times the desired output the features.

\begin{eqnarray*}
  L'_{\ell+1}[x,y,i] & = & \sigma\left(W[\Delta X, \Delta Y, J,i]\; L'_\ell[x + \Delta X, y + \Delta Y, J]\right)
\end{eqnarray*}

\vfill
We then reshape $L'_{\ell+1}[X,Y,I]$ to $L'_{\ell+1}[2X,2Y,I/4]$.

\slide{Rounding is not Differentiable}

$$\Phi^* = \argmin_\Phi \;E_{y \sim \pop}\;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$

\vfill
Because of rounding, $\tilde{z}_\Phi(y)$ is discrete and the gradients are zero.

\vfill
We will train using a differentiable approximation.

\slide{Rate: Replacing Code Length with Differential Entropy}

\begin{eqnarray*}
{\cal L}_{\mathrm{rate}}(\Phi) & = & E_{y \sim \pop}\;|\tilde{z}_\Phi(y)|
\end{eqnarray*}

\vfill
Recall that {\color{red} $\tilde{z}_\Phi(y)$} is a rounding of a continuous encoding {\color{red} $z_\Phi(y)$}.

\vfill
We approximate the code length after rounding using a differentiable function of the value before rounding.

\vfill
{\color{red} $$|{\color{red} \tilde{z}_\Phi(y)}| \approx \sum_{x,y,i} \log_2({\color{red} z_\Phi(y)[x,y,i]})^+$$}

This continuous value can be interpreted as a ``differential entropy'' with $p(z) \propto 1/z$ for some finite range of $z$.

\slide{Distortion: Replacing Rounding with Noise}

We can make distortion differentiable by modeling rounding as the addition of noise.

\begin{eqnarray*}
{\cal L}_{\mathrm{dist}}(\Phi) & = & E_{y \sim \mathrm{Pop}} \;\mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y))) \\
\\
& \approx & E_{y,\epsilon} \;\mathrm{Dist}(y,\;y_\Phi(z_\Phi(y) + \epsilon))
\end{eqnarray*}

\vfill
Here $\epsilon$ is a noise vector each component of which is drawn uniformly from $(-1/2,1/2)$.

\slide{Rate: Differential Entropy vs. Discrete Entropy}

\bigskip
\centerline{\includegraphics[height=3in]{../images/RateDist6}}

Each point is a rate for an image measured in both differential entropy and discrete entropy.  The size of the rate changes as we change the weight $\lambda$.

\slide{Distortion: Noise vs. Rounding}

\centerline{\includegraphics[height=3in]{../images/RateDist5}}

Each point is a distortion for an image measured in both a rounding model and a noise model.  The size of the distortion changes as we change the weight $\lambda$.

\anaslide{JPEG at 4283 bytes or .121 bits per pixel}

\bigskip
\centerline{\includegraphics[height=5in]{../images/RateDist2}}

\anaslide{JPEG 2000 at 4004 bytes or .113 bits per pixel}

\bigskip
\centerline{\includegraphics[height= 5in]{../images/RateDist3}}

\anaslide{Deep Autoencoder at 3986 bytes or .113 bits per pixel}

\bigskip
\centerline{\includegraphics[height = 5in]{../images/RateDist4}}

\slide{Noisy-Channel RDAs}

The image compression case study training was based on a differentiable loss which can be written in the form

\vfill
$$\Phi^* = \argmin_\Phi \left(\;{\color{red} E_y\;- \ln p_\Phi(z_\Phi(y))}\right) + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(z_\Phi(y) + \epsilon))$$

\vfill
In a rate-distortion auto-encoder we will measure rate directly on continuous variables.

\vfill
The problem is that the first term --- the cross entropy term --- should be viewed as being infinite --- there are infinitely many bits in a real number.

\slide{Mutual Information Replaces Cross Entropy}

We replace
\vfill
\vfill
$$\Phi^* = \argmin_\Phi \left(\;{\color{red} E_y\;- \ln p_\Phi(z_\Phi(y))}\right) + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(z_\Phi(y) + \epsilon))$$

by

\begin{eqnarray*}
\tilde{z} & = & z_\Phi(y) + \epsilon \;\;\mbox{($\epsilon$ is random noise --- typically Gaussian)} \\
\\
\Phi^* & = & \argmin_\Phi \;{\color{red} I(y,\tilde{z})} + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(\tilde{z}))
\end{eqnarray*}

\vfill
By the channel capacity theorem {\color{red} $I(y,\tilde{z})$} is the {\bf rate} of information transfer from $y$ to $\tilde{z}$.

\slide{Mutual Information Replaces Cross Entropy}

\begin{eqnarray*}
I(y,\tilde{z})  & = & E_{y,\tilde{z}}\; \ln \frac{p(y,\tilde{z})}{p(\tilde{z})p(y)} \\
\\
\\
\\
& = & E_{y,\tilde{z}}\;\ln \frac{p(\tilde{z}\;|\;z_\Phi(y))}{p(\tilde{z})}
\end{eqnarray*}

\slide{A Variational Bound}

$$p(\tilde{z}) = E_y\;p(\tilde{z}\;|\;z_\Phi(y))$$

\vfill
We cannot compute $p(\tilde{z})$.

\vfill
Instead we have a model $p_\Phi(\tilde{z})$.

\vfill
The model corresponds to the ``code'' we are using to approximate the true distribution $p(\tilde{z})$.

\slide{A Variational Bound}

\begin{eqnarray*}
{\color{red} I(y,\tilde{z})}  & = & E_{y,\tilde{z}}\;\ln \frac{p(\tilde{z}\;|\;z_\Phi(y))}{p(\tilde{z})} \\
\\
& = & E_{y,\tilde{z}}\;\ln \frac{p(\tilde{z}\;|\;z_\Phi(y))}{p_\Phi(\tilde{z})} + E_{\tilde{z}}\;\ln\frac{p_\Phi(\tilde{z})}{p(\tilde{z})} \\
\\
& = & E_{y,\tilde{z}}\;\ln \frac{p(\tilde{z}\;|\;z_\Phi(y))}{p_\Phi(\tilde{z})} - KL(p(\tilde{z}),p_\Phi(\tilde{z})) \\
\\
& {\color{red} \leq} & {\color{red} E_{y,\tilde{z}}\;\ln \frac{p(\tilde{z}\;|\;z_\Phi(y))}{p_\Phi(\tilde{z})}}
\end{eqnarray*}


\slide{Cross MI}

\begin{eqnarray*}
{\color{red} I(y,\tilde{z})} & {\color{red} \leq} & {\color{red} E_{y,\tilde{z}}\;\ln \frac{p(\tilde{z}\;|\;y)}{p_\Phi(\tilde{z})}}
\end{eqnarray*}

\vfill
We might call the right hand side ``cross MI'' written $I(y,\tilde{z},p_\Phi)$.

\vfill
Cross MI, unlike true MI, is measurable.

\slide{A Fundamental Equation for the  Continuous Case}

\begin{eqnarray*}
\tilde{z} & = & z_\Phi(y) + \epsilon \;\;\mbox{($\epsilon$ is random noise --- typically Gaussian)} \\
\\
\Phi^* & = & \argmin_\Phi E_{y,\tilde{z}}\; \;\ln \frac{p(\tilde{z}|z_\Phi(y))}{p_\Phi(\tilde{z})} + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}))
\end{eqnarray*}

\slide{Gaussian Noisy-Channel RDA}

$$\Phi^* = \argmin_{\Phi}\;E_{y\sim \pop} \left(\begin{array}{l}\;\;\;\;KL(p_\Phi(\tilde{z}|y),p_\Phi(\tilde{z})) \\
\\
+ \lambda \; E_{\tilde{z}\sim p_\Phi(\tilde{z}|y)}\;\mathrm{Dist}(y,\;y_\Phi(\tilde{z}))\end{array}\right)$$

{\color{red}
\begin{eqnarray*}
p_\Phi(\tilde{z}[i]\;|\;y) & = & {\cal N}(z_\Phi(y)[i],\sigma^\epsilon_\Phi(y)[i])) \\
\\
p_\Phi(\tilde{z}[i]) & = & {\cal N}(\mu_\Phi[i],\sigma^z_\Phi[i]) \\
\\
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2
\end{eqnarray*}
}

\slide{Closed Form KL-Divergence}

\begin{eqnarray*}
& & KL(p_\Phi(\tilde{z}|y),p_\Phi(\tilde{z})) \\
\\
\\
& = & \sum_i \;\frac{\sigma^\epsilon_\Phi(y)[i]^2 + (z_\Phi(y)[i]-\mu_\Phi[i])^2}{2 \sigma^z_\Phi[i]^2}
+ \ln\frac{\sigma^z_\Phi[i]}{\sigma^\epsilon_\Phi(y)[i]} - \frac{1}{2}
\end{eqnarray*}


\slide{Standardizing $p_\Phi(z)$}

\begin{eqnarray*}
 &  & KL(p_\Phi(\tilde{z}|y),p_\Phi(\tilde{z})) \\
 \\
 & = & \sum_i \;\frac{ \sigma^\epsilon_\Phi(y)[i]^2 +(z_\Phi(y)[i] - \mu_\Phi[i])^2}{2\sigma^z_\Phi[i]^2}
+ \ln\frac{\sigma^z_\Phi[i]}{\sigma^\epsilon_\Phi(y)[i]}
- \frac{1}{2}
\\
\\
\\
\\
 &  & KL(p_{\Phi'}(\tilde{z}|y),{\cal N}(0,I)) \\
 \\
 & = & \sum_i \;\frac{\sigma^\epsilon_{\Phi'}(y)[i]^2 +z_{\Phi'}(y)[i]^2}{2} + \ln\frac{1}{\sigma^\epsilon_{\Phi'}(y)[i]} - \frac{1}{2}
\end{eqnarray*}

\slide{Standardizing $p_\Phi(z)$}

\begin{eqnarray*}
KL_\Phi & = & \sum_i \;\frac{ \sigma^\epsilon_\Phi(y)[i]^2 +(z_\Phi(y)[i] - \mu_\Phi[i])^2}{2\sigma^z_\Phi[i]^2}
+ \ln\frac{\sigma^z_\Phi[i]}{\sigma^\epsilon_\Phi(y)[i]} - \frac{1}{2}
\\
KL_{\Phi'} & = & \sum_i \;\frac{\sigma^\epsilon_{\Phi'}(y)[i]^2 +z_{\Phi'}(y)[i]^2}{2} + \ln\frac{1}{\sigma^\epsilon_{\Phi'}(y)[i]} - \frac{1}{2}
\end{eqnarray*}

Setting $\Phi'$ so that
\begin{eqnarray*}
z_{\Phi'}(y)[i] & = & (z_\Phi(y)[i] - \mu_\Phi[i])/\sigma^z_\Phi[i] \\
\sigma^\epsilon_{\Phi'}(y)[i] & = & \sigma^\epsilon_\Phi(y)[i]/\sigma^z_\Phi[i]
\end{eqnarray*}

\vfill
gives {\color{red} $KL(p_{\Phi}(\tilde{z}|y),p_\Phi(\tilde{z})) = KL(p_{\Phi'}(\tilde{z}|y),{\cal N}(0,I))$}.

\slide{Standardizing $p_\Phi(z)$}

\vfill
Without loss of generality the Gaussian noisy channel RDA becomes.

{\color{red} $$\Phi^* = \argmin_{\Phi}\; E_{y \sim \pop}\left(\begin{array}{l}\;\;\;\;KL(p_\Phi(\tilde{z}|y),{\cal N}(0,I)) \\
\\
+ \lambda E_{\tilde{z} \sim p_\Phi(\tilde{z}|y)}\;\mathrm{Dist}(y,y_\Phi(\tilde{z}))\end{array}\right) $$}

\slide{Reparameterization Trick for Optimizing Distortion}

$$p_\Phi(\tilde{z}[i]|y) = {\cal N}(z_\Phi(y)[i],\sigma_\Phi[i])$$

\vfill
\begin{eqnarray*}
& & E_{\color{red} \tilde{z} \sim p_\Phi(\tilde{z}|y)} \;||y - y_\Phi(\tilde{z})||^2 \\
\\
\\
& = & E_{\color{red} \epsilon \sim {\cal N}(0,I)} \;{\color{red} \tilde{z}[i] = z_\Phi(y)[i] + \sigma_\Phi(y)[i]\epsilon[i]};\;\; \;||y -y_\Phi(\tilde{z})||^2
\end{eqnarray*}

\slide{Sampling}

\centerline{Sample {\color{red} $\tilde{z} \sim {\cal N}(0,I)$} and compute {\color{red} $y_\Phi(\tilde{z})$}}
\vfill
\centerline{\includegraphics[width = 4in]{../images/VariationalFaces}}
\centerline{[Alec Radford]}


\slide{Summary: Rate-Distortion}

RDA: $y$ continuous, $\tilde{z}$ a bit string,

{\color{red}
\begin{eqnarray*}
\Phi^* &  = &  \argmin_\Phi E_{y \sim \pop} \;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))
\end{eqnarray*}
}

\vfill
Gaussian RDA: {\color{red} $\tilde{z} = z_\Phi(y) + \sigma_\Phi(y) \odot \epsilon$,\hspace{2em} $\epsilon \sim {\cal N}(0,I)$}

{\color{red}
\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi E_{y \sim \pop}\left(\begin{array}{l}\;\;\; \;KL(p_\Phi(\tilde{z}|y),{\cal N}(0,I)) \\ \\ + \lambda E_{\tilde{z}\sim p_\Phi(\tilde{z}|y)}\; \mathrm{Dist}(y,y_\Phi(\tilde{z})) \end{array}\right)
\end{eqnarray*}
}

\vfill
Issue: Do we expect compression to yield useful features?

\slide{END}

}
\end{document}


\slide{Gaussian Variational Autoencoder (Gaussian VAE)}

{\huge  $$\Phi^* = \argmin_{\Phi}\;E_{y\sim \pop} \left(\begin{array}{l}\;\;\;\;\;\;KL(p_\Phi(z|y),{\cal N}(0,I)) \\
\;+\; \; {\color{red} \frac{1}{2(\sigma^*(u))^2}} E_{z\sim p_\Phi(z|y)}\;\mathrm{Dist}(y,\;y_\Phi(z))\end{array}\;\right)$$

\begin{eqnarray*}
z[i] & = & z_\Phi(y)[i] + \sigma_\Phi(y)[i]\epsilon\;\;\;\;\epsilon \sim {\cal N}(0,1) \\
p_\Psi(z[i]) & = & {\cal N}(\mu_\Psi[i],\sigma_\Psi[i]) \\
\mathrm{Dist}(y,\hat{y}) & = & ||y-\hat{y}||^2 \\
\\
{\color{red} \sigma^*(u)} & {\color{red} =} & {\color{red} \argmin_\sigma E_{y \sim \pop,z \sim p_\Phi(z|y)}\; \frac{1}{2\sigma^2}\mathrm{Dist}(y,y_\Phi(z)) + d \ln \frac{\sigma}{u}}
\;\mbox{for $y \in \mathbb{R}^d$}
\end{eqnarray*}

\vfill
\centerline{{\color{red} $u$} is the unit of measure for the components of $y$}

