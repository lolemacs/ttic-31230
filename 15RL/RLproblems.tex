\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\bigskip
\centerline{\bf RL Problems.}

\bigskip
\bigskip
{\bf Problem 1.}
Consider training machine translation on a corpus of translation pairs $(x,y)$ where $x$ is, say, an English sentence $x_1,\ldots,\mathrm{EOS}$ and $y$ is a French sentence $y_1,\ldots,\mathrm{EOS}$
where EOS is the ``end of sentence'' tag.

\medskip
Suppose that we have a parameterized model defining $P_\Phi(y_t|x,y_1,\ldots,y_{t-1})$ so that $P_\Phi (y_1,\ldots,y_{T}|x) = \prod_{t=1}^{T'}P_\Phi(y_t|x,y_1,\ldots,y_{t-1})$ where $y_T$ is EOS.

\medskip
For a sample $\hat{y}$ from $P_\Phi(y|x)$ we have a non-differentiable BLEU score $\mathrm{BLEU}(\hat{y},y) \geq 0$ that is not computed until the entire output $y$ is complete
and which we would like to maximize.

\medskip
(a) Give an SGD update equation for the parameters $\Phi$ for the REINFORCE algorithm for maximizing $E_{\hat{y} \sim P_\Phi(y|x)}$ for this problem.

\solution{
  For $\tuple{x,y}$ samples form the training corpus of translation pairs, and for $\hat{y}_1,\ldots,\hat{y}_T$ sampled from $P_\Phi(\hat{y}|x)$
  we udate $\Phi$ by
  $$ \Phi \; \pluseq \; \eta\mathrm{BLEU}(\hat{y},y)\sum_{t = 1}^T \; \nabla_\Phi\;\ln P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1})$$
  Samples with higher BLEU scores have their probabilities increased.
  }

\medskip
(b) Suppose that somehow we reach a parameter setting $\Phi$ where $P_\Phi(y|x)$ assigns probability close enough to 1 for a particular translation $\hat{y}$
that in practice we will always sample the same $\hat{y}$.  Suppose that this translation $\hat{y}$ has less than optimal BLEU score.
Can the REINFORCE algorithm recover from this situation and consider other translations?  Explain your answer.

\solution{No.  The REINFORCE algorithm will not recover. The update will only increase the probability of the single translation which it always selects.
A deterministic policy has zero gradient and is stuck.}

\medskip
(c)  Show that for any function $V(x)$ we have
$$E_{\hat{y} \sim P_\Phi(\hat{y}|x)}\;\;\;V(x) \nabla_\Phi \ln P_\Phi(\hat{y}_t|x,y_1,\ldots,y_{t-1}) = 0$$

\solution{
  \begin{eqnarray*}
    & & E_{\hat{y}}\;V(x) \nabla_\Phi \ln P_\Phi(\hat{y}_t|x,y_1,\ldots,y_{t-1}) \\
    \\
    & = & V(x) E_{\hat{y}_1\ldots,\hat{y}_{t-1}}\; \sum_{\hat{y}_t} P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1})
    \frac{\nabla_\Phi\;P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1})}{P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1})} \\
    \\
    & = & V(x) E_{\hat{y}_1\ldots,\hat{y}_{t-1}}\; \sum_{\hat{y}_t} P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1})
    \frac{\nabla_\Phi\;P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1})}{P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1})} \\
    \\
    & = & V(x) E_{\hat{y}_1\ldots,\hat{y}_{t-1}}\; \nabla_\Phi \sum_{\hat{y}_t} P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1}) \\
    \\
    & = & 0
  \end{eqnarray*}
      }
        

\medskip
(d) Modify the REINFORCE update equations to use a value function approximation $V_\Phi(x)$ to reduce the variance in the gradient samples and where
$V_\Phi$ is trained by Bellman Error.
Your equations should include updates to train
$V_\Phi(x)$ to predict $E_{\hat{y} \sim P(y|x)}\;\mathrm{BLEU}(\hat{y},y)$.  (Replace the reward by the ``advantage'' of the particular translation).

\solution{
  For $\tuple{x,y}$ sampled form the training corpus of translation pairs, and for $\hat{y}_1,\ldots,\hat{y}_T$ sampled from $P_\Phi(\hat{y}|x)$
  we udate $\Phi$ by
  \begin{eqnarray*}
    \Phi & \pluseq & \eta(\mathrm{BLEU}(\hat{y},y)-V_\Phi(x))\sum_{t = 1}^T \; \nabla_\Phi\;\ln P_\Phi(\hat{y}_t|x,\hat{y}_1,\ldots,\hat{y}_{t-1}) \\
    \\
    \Phi & \minuseq & 2\eta (V_\Phi(x) - \mathrm{BLEU}(\hat{y},y))
    \end{eqnarray*}
}

\end{document}
