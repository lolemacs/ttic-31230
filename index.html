 <title>TTIC 31230: Fundamentals of Deep Learning</title>

<header>TTIC 31230: Fundamentals of Deep Learning</header>

<p> David McAllester</p>

<p> Revised from winter 2020</p>

<!-- <p style="color:red"> Last Lecture Canceled.  Prof. McAllester appears to have some mild illness and it seems best to err on the side of the safety of the students.</p> -->

<p>Lectures Slides and Course Material:</p>

<ol>

  <li> Introduction</li>

  <ol type = "A">
    <li><a href = 01intro/history.pdf> The History of Deep Learning and Moore's Law of AI</a></li>

    <li><a href = 01intro/fundamentals.pdf> The Fundamental Equations of Deep Learning</a></li>

    <li><a href = 01intro/problems.pdf>Problems</a></li>

  </ol>

  <li> Frameworks and Back-Propagation</li>

  <ol type = "A">

    <li><a href = 02MLP/frameworks.pdf> Deep Learning Frameworks</a></li>

    <li><a href = 02MLP/Backprop.pdf> Backpropagation for Scalar Source Code</a></li>

    <li><a href = 02MLP/backprop2.pdf> Backpropagation for Tensor Source Code</a></li>

    <li><a href = 02MLP/minibatching.pdf>  Minibatching: The Batch Index</a></li>

    <li><a href = 02MLP/EDFslides.pdf>  The Educational Framework (EDF)</a></li>
        
    <li><a href = 02MLP/problems.pdf> Problems</a></li>

    <li><a href = 02MLP/edf.py> EDF source code</a> 150 lines of Python/NumPy</li>
    
    <li><a href = 02MLP/PS1.zip> MNIST in EDF problem set</a></li>

    <li><a href = https://pytorch.org/tutorials/ > PyTorch tutorial</a></li>

  </ol>

  <li>Vision: Convolutional Neural Networks (CNNs)</li>

  <ol type = "A">
    <li><a href = 03CNNs/Einstein.pdf> Einstein Notation</li>
    <li><a href = 03CNNs/CNNs.pdf> CNNs</li>
    <li><a href = 03CNNs/trainability.pdf> Trainability: Relu, Initialization, Batch Normalization and Residual Connections (ResNet)</li>
    <li><a href = 03CNNs/CNNb.html> Invariant Theory (optional)</li>
    <li><a href = 03CNNs/problems.pdf> Problems</a></li>
    <li><a href = https://pytorch.org/docs/stable/nn.functional.html?highlight=convolution>Pytorch Convolution Functions</a></li>
  </ol>

  <li> Natural Language Processing</li>

  <ol type = 'A'>

    <li><a href = 05Rnns/LangModels.pdf> Language Modeling</a></li>
    <li><a href = 05RNNs/RNNs.pdf> Recurrent Neural Networks (RNNs)</a></li>
    <li><a href = 05RNNs/Translation.pdf> Machine Translation and Attention</a></li>
    <li><a href = 05RNNs/Transformer.pdf> The Transformer</a></li>
    <li><a href = 05RNNs/Phrases.pdf> Statistical Machine Translation (optional)</a></li>

    <li><a href = 05RNNs/problems.pdf>Problems</a></li>
    
    <li>References</li>

    <ol type = "i">
      <li><a href = https://arxiv.org/abs/1409.3215> Original sequence to sequence paper </a></li>

      <li><a href = https://arxiv.org/abs/1409.0473> Original attention paper </a></li>

      <li><a href = https://arxiv.org/abs/1611.04558> Google's Revolution in Machine Translation </a></li>
    
      <li><a href = https://arxiv.org/abs/1706.03762> Attention is all you need</a></li>
      </ol>

  </ol>

  <li>Stochastic Gradient Descent</li>

  <ol type = "A">
    
    <li><a href = 06SGD/Classical.pdf> The Classical Convergence Theorem</a></li>

    <li><a href = 06SGD/Decoupling1.pdf> Decoupling the Learning Rate from the Batch Size</a></li>

    <li><a href = 06SGD/Momentum.pdf> Momentum as a Running Average and Decoupled Momentum</a></li>

    <li><a href = 06SGD/RMS.pdf> RMSProp, and Adam and Decoupled Versions</a></li>
    
    <li><a href = 06SGD/flow.pdf> Gradient Flow</a></li>

    <li><a href = 06SGD/Heat.pdf> Heat Capacity: Loss as Energy and Learning Rate as Temperature</a></li>

    <li><a href = 06SGD/Langevin.pdf> Continuous Time Noise</a></li>

    <li><a href = 06SGD/SGDproblems.pdf> Problems</a></li>

    <!-- <li><a href = 06SGD/safe.pdf> Slides on a Quenching Algorithm</a></li> -->

    <li> References: </li>
    <ol type = "i">
      <li><a href = http://ruder.io/optimizing-gradient-descent/ > Blog post on SGD variants</a></li>

      <li><a href = https://arxiv.org/abs/1706.02677> Training Resnt-50 on Imagenet in one hour</a></li>
    
      <li><a href = https://openreview.net/pdf?id=B1Yy1BxCZ > Paper on batch size scaling of the learning rate and momentum parameter</a></li>

      <li><a href = https://arxiv.org/abs/1511.06807> Adding Gradient Noise</a></li>
    
      <li><a href = https://arxiv.org/abs/1704.00109> Temperature Cycling in SGD </a></li>
      
      <li><a href = https://arxiv.org/abs/1206.1901> MCMC with momentum</a></li>
    </ol>
  </ol>

  <li><a href = 07regularization/regularization.html> Generalization and Regularization</li>

  <ol type= 'A'>
    <li><a href = regularization.pdf> Slides</a></li>

    <li><a href = REGproblems.pdf> Problems</a></li>

    <li> References: </li>

    <ol type = "i">
      <li><a href =  https://arxiv.org/abs/1307.2118> PAC-Bayes Tutorial </a></li>
      
      <li><a href = http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks> The Alexnet Paper Demonstraing Dropout (2012) </a></li>

      <li><a href = http://jmlr.org/papers/v15/srivastava14a.html> The Journal Dropout Reference (2014)  </a></li>

      <li><a href = https://arxiv.org/abs/1512.05287> Effective Dropout for RNNs (2015) </a></li>

      <li><a href = https://openreview.net/pdf?id=Sy8gdB9xx> Paper on Over-Parameterization (2017) </a></li>
    
      <li><a href = https://arxiv.org/abs/1703.11008> Non-Vacuous PAC-Bayes Bounds (2017) </a></li>
    
      <li><a href =  https://arxiv.org/abs/1804.05862> Non-Vacuous PAC-Bayes Bounds at ImageNet Scale (2018) </a></li>

      <li><a href = https://arxiv.org/abs/1812.11118> Double Descent Paper (2019) </a></li>

      <li><a href = https://arxiv.org/abs/1912.02292> Follow-up Double Descent Paper (2020) </a></li>
    </ol>
  </ol>
  
  <li><a href = 09GraphicalModels/CTC.html> Connectionist Temporal Classification (CTC)</li>

  <li><a href = 09GraphicalModels/DGMs.html> Deep Graphical Models</li>

  <li><a href = 08InfoTheory/information.html> More Information Theory: Perils of Differential Entropy </li>
  
  <li><a href = 14GANs/GANs.html> Generative Adversarial Networks (GANs)</a></li>

  <li><a href = 11AutoEncoders/Rate.html> Rate-Distortion Autoencoders (RDAs)</li>

  <li><a href = 11AutoEncoders/Variational.html> Variational Autoencoders (VAEs)</li>

  <li><a href = pretraining/pretraining.html> Pretraining</a></li>

  <li><a href = 15RL/RL.html> Reinforcement Learning (RL)</a></li>

  <li><a href = 16alpha/alpha.html> AlphaZero</a></li>

  <li><a href = 13SGD2/SGD2.html> Gradients as Dual Vectors, Hessian-Vector Products, and Information Geometry </a></li>

  <li><a href = 17Interpretation/Interp.html> The Black Box Problem</a></li>

  <li><a href = 18AGI/AGI.html> The Quest for Artificial General Intelligence (AGI)</a></li>
  
</ol>

<ol>
  <li><a href = quiz2-20/quiz2.pdf> Quiz 2</a></li>
  <li><a href = quiz3-20/quiz3.pdf> Quiz 3</a></li>
  <li><a href = quiz4/quiz4.pdf> Quiz 4</a></li>
  <li><a href = quiz5/quiz5.pdf> Quiz 5</a></li>
</ol>

<p> Each quiz will occupy the last 2/3 of one class.  The quizes are scheduled two weeks apart as follows: </p>

<ul>
  <li> Quiz 2, Tuesday January 28 covers lectures 3,4, and 5.</li>
  <li> Quiz 3, Tuesday February 11 covers lectures 6, 7 and 8 </li>
  <li> Quiz 4, Tuesday February 25 covers lectures 10, 11 and 12 </li>
  <li> Quiz 5, Tuesday March 10 </li>
</ul>

There is no final.

<p> Office hours will be Monday and Friday 2:00 to 4:00 in the library (TTIC 435).</p>

  


  


