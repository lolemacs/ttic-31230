 <title>TTIC 31230: Fundamentals of Deep Learning</title>

<header>TTIC 31230: Fundamentals of Deep Learning</header>

<p> David McAllester</p>

<p> Revised from winter 2020</p>

<!-- <p style="color:red"> Last Lecture Canceled.  Prof. McAllester appears to have some mild illness and it seems best to err on the side of the safety of the students.</p> -->

<p>Lectures Slides and Course Material:</p>

<ol>

  <li> Introduction</li>

  <ol type = "A">
    <li><a href = 01intro/history.pdf> The History of Deep Learning and Moore's Law of AI</a></li>

    <li><a href = 01intro/fundamentals.pdf> The Fundamental Equations of Deep Learning</a></li>

    <li><a href = 01intro/problems.pdf>Problems</a></li>

  </ol>

  <li> Frameworks and Back-Propagation</li>

  <ol type = "A">

    <li><a href = 02MLP/frameworks.pdf> Deep Learning Frameworks</a></li>

    <li><a href = 02MLP/Backprop.pdf> Backpropagation for Scalar Source Code</a></li>

    <li><a href = 02MLP/backprop2.pdf> Backpropagation for Tensor Source Code</a></li>

    <li><a href = 02MLP/minibatching.pdf>  Minibatching: The Batch Index</a></li>

    <li><a href = 02MLP/EDFslides.pdf>  The Educational Framework (EDF)</a></li>
        
    <li><a href = 02MLP/problems.pdf> Problems</a></li>

    <li><a href = 02MLP/edf.py> EDF source code</a> 150 lines of Python/NumPy</li>
    
    <li><a href = 02MLP/PS1.zip> MNIST in EDF problem set</a></li>

    <li><a href = https://pytorch.org/tutorials/ > PyTorch tutorial</a></li>

  </ol>

  <li>Vision: Convolutional Neural Networks (CNNs)</li>

  <ol type = "A">
    <li><a href = 03CNNs/Einstein.pdf> Einstein Notation</li>
    <li><a href = 03CNNs/CNNs.pdf> CNNs</li>
    <li><a href = 03CNNs/trainability.pdf> Trainability: Relu, Initialization, Batch Normalization and Residual Connections (ResNet)</li>
    <li><a href = 03CNNs/CNNb.html> Invariant Theory (optional)</li>
    <li><a href = 03CNNs/problems.pdf> Problems</a></li>
    <li><a href = https://pytorch.org/docs/stable/nn.functional.html?highlight=convolution>Pytorch Convolution Functions</a></li>
  </ol>

  <li> Natural Language Processing</li>

  <ol type = 'A'>

    <li><a href = 05Rnns/LangModels.pdf> Language Modeling</a></li>
    <li><a href = 05RNNs/RNNs.pdf> Recurrent Neural Networks (RNNs)</a></li>
    <li><a href = 05RNNs/Translation.pdf> Machine Translation and Attention</a></li>
    <li><a href = 05RNNs/Transformer.pdf> The Transformer</a></li>
    <li><a href = 05RNNs/Phrases.pdf> Statistical Machine Translation (optional)</a></li>

    <li><a href = 05RNNs/problems.pdf>Problems</a></li>

    <!--
    <li>References</li>

    <ol type = "i">
      <li><a href = https://arxiv.org/abs/1409.3215> Original sequence to sequence paper </a></li>

      <li><a href = https://arxiv.org/abs/1409.0473> Original attention paper </a></li>

      <li><a href = https://arxiv.org/abs/1611.04558> Google's Revolution in Machine Translation </a></li>
    
      <li><a href = https://arxiv.org/abs/1706.03762> Attention is all you need</a></li>
      </ol>
    -->
    
  </ol>

  <li>Stochastic Gradient Descent</li>

  <ol type = "A">
    
    <li><a href = 06SGD/Classical.pdf> The Classical Convergence Theorem</a></li>

    <li><a href = 06SGD/Decoupling1.pdf> Decoupling the Learning Rate from the Batch Size</a></li>

    <li><a href = 06SGD/Momentum.pdf> Momentum as a Running Average and Decoupled Momentum</a></li>

    <li><a href = 06SGD/RMS.pdf> RMSProp, and Adam and Decoupled Versions</a></li>
    
    <li><a href = 06SGD/flow.pdf> Gradient Flow</a></li>

    <li><a href = 06SGD/Heat.pdf> Heat Capacity with Loss as Energy and Learning Rate as Temperature</a></li>

    <li><a href = 06SGD/Langevin.pdf> Continuous Time Noise and Stationary Parameter Densities</a></li>

    <li><a href = 06SGD/SGDproblems.pdf> Problems</a></li>

    <!-- <li><a href = 06SGD/safe.pdf> Slides on a Quenching Algorithm</a></li> -->

    <!--
    <li> References: </li>
    <ol type = "i">
      <li><a href = http://ruder.io/optimizing-gradient-descent/ > Blog post on SGD variants</a></li>

      <li><a href = https://arxiv.org/abs/1706.02677> Training Resnt-50 on Imagenet in one hour</a></li>
    
      <li><a href = https://openreview.net/pdf?id=B1Yy1BxCZ > Paper on batch size scaling of the learning rate and momentum parameter</a></li>

      <li><a href = https://arxiv.org/abs/1511.06807> Adding Gradient Noise</a></li>
    
      <li><a href = https://arxiv.org/abs/1704.00109> Temperature Cycling in SGD </a></li>
      
      <li><a href = https://arxiv.org/abs/1206.1901> MCMC with momentum</a></li>
    </ol> -->
  </ol>
  
  <li>Generalization and Regularization</li>

  <ol type= 'A'>
    <li><a href = 07regularization/Early.pdf>Early Stopping, Shrinkage and Decoupled Shrinkage</a></li>
    <li><a href = 07regularization/PCABayes.pdf>PAC-Bayes Generalization Theory</a></li>
    <li><a href = 07regularization/Implicit.pdf>Implicit Regularization</a></li>
    <li><a href = 07regularization/Double.pdf>Double Descent</a></li>
    <li><a href = 07regularization/REGproblems.pdf> Problems</a></li>
    <li><a href =  https://arxiv.org/abs/1307.2118> PAC-Bayes Tutorial </a></li>
    <li><a href = https://arxiv.org/abs/1703.11008> Non-Vacuous PAC-Bayes Bounds (2017) </a></li>
    <li><a href =  https://arxiv.org/abs/1804.05862> Non-Vacuous PAC-Bayes Bounds at ImageNet Scale (2018) </a></li>
    <li><a href = https://arxiv.org/abs/1812.11118> Double Descent Paper (2019) </a></li>
    <li><a href = https://arxiv.org/abs/1912.02292> Follow-up Double Descent Paper (2020) </a></li>
  </ol>
  
  <li>Deep Graphical Models</li>

  <ol type = 'A'>
    <li><a href = 09GraphicalModels/DGMs1.pdf> Exponential Softmax</a></li>
    
    <li><a href = 09GraphicalModels/CTC.pdf> Speech Recognition: Connectionist Temporal Classification (CTC)</a></li>

    <li><a href = 09GraphicalModels/DGMs2.pdf> Backprogation for Exponential Softmax: The Model Marginals</a></li>

    <li><a href = 09GraphicalModels/MCMC.pdf> Monte-Carlo Markov Chain (MCMC) Sampling</a></li>

    <li><a href = 09GraphicalModels/MCMC.pdf> Pseudo-Likelihood and Contrastive Divergence</a></li>
    
    <li><a href = 09GraphicalModels/Loopy.pdf> Loopy Belief Propagation (Loopy BP)</a></li>

    <li><a href = 09GraphicalModels/CTCproblems.pdf> CTC Problems</a></li>
    
    <li><a href = 09GraphicalModels/DGMproblems.pdf> Algorithms Problems</a></li>

  </ol>

  <li>Generative Adversarial Networks (GANs)</li>

  <ol>
    <li><a href = 14Gans/Gans.pdf> Overview and Timeline of GAN Development</a></li>

    <li><a href = 14Gans/Patch.pdf>Replacing the Loss Gradient with the Margin Gradient.</a></li>

    <li><a href = 14Gans/Jensen.pdf>Optimal Discrimination and Jensen-Shannon Divergence</a></li>

    <li><a href = 14Gans/Contrastive.pdf>Contrastive GANs</a></li>

    <li><a href = 14GANs/GANproblems.pdf> Problems</a></li>

  </ol>

  <li>Autoencoders</li>
  
  <ol>
    <li><a href = 08InfoTheory/information.pdf>Perils of Differential Entropy</a></li>

    <!-- <li><a href = 08InfoTheory/info2problems.pdf> Problems /a></li> -->

    <li><a href = 11AutoEncoders/Rate.pdf> Rate-Distortion Autoencoders (RDAs) </a></li>

    <li><a href = 11AutoEncoders/Noisy.pdf> Noisy Channel RDAs </a></li>

    <li><a href = 11AutoEncoders/GaussianRDAs.pdf> Gaussian Noisy Channel RDAs </a></li>

    <li><a href = 11AutoEncoders/Latent.pdf> Assumptions on Latent Variable Models</a></li>

    <li><a href = 11AutoEncoders/ELBO.pdf> The Evidence Lower Bound (ELBO) </a></li>

    <li><a href = 11AutoEncoders/Variational.pdf> Variational Autoencoders (VAEs) </a></li>

    <li><a href = 11AutoEncoders/Variational.pdf> Variational Autoencoders (VAEs) </a></li>

    <li><a href = 11AutoEncoders/Rateproblems.pdf> Problems for RDAs and VAEs</a></li>
  </ol>
  
  <li><a href = pretraining/pretraining.html> Pretraining</a></li>

  <li><a href = 15RL/RL.html> Reinforcement Learning (RL)</a></li>

  <li><a href = 16alpha/alpha.html> AlphaZero</a></li>

  <li><a href = 13SGD2/SGD2.html> Gradients as Dual Vectors, Hessian-Vector Products, and Information Geometry </a></li>

  <li><a href = 17Interpretation/Interp.html> The Black Box Problem</a></li>

  <li><a href = 18AGI/AGI.html> The Quest for Artificial General Intelligence (AGI)</a></li>
  
</ol>

<ol>
  <li><a href = quiz2-20/quiz2.pdf> Quiz 2</a></li>
  <li><a href = quiz3-20/quiz3.pdf> Quiz 3</a></li>
  <li><a href = quiz4/quiz4.pdf> Quiz 4</a></li>
  <li><a href = quiz5/quiz5.pdf> Quiz 5</a></li>
</ol>

  


  


