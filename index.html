 <title>TTIC 31230: Fundamentals of Deep Learning</title>

<header>TTIC 31230: Fundamentals of Deep Learning</header>

<p> David McAllester</p>

<p> Winter 2020</p>

<p> TA: Adam Dziedzic</p>
<p> Grader: Gray Mackall </p>

<!-- <p style="color:red"> Last Lecture Canceled.  Prof. McAllester appears to have some mild illness and it seems best to err on the side of the safety of the students.</p> -->

<p> In 2020, as in 2019, there will be no machine problems or class projects.  The course will again be treated as an "algorithms class" rather than a "programming class".</p>

<p> The grade will be based entirely on five equally weighted in-class quizes.  Each lecture comes with paractice problems with solutions.
The quizes will include problems from the practice set --- you just need to memorize the solution --- as well as original problems.</p>

<p>Lectures Slides and Course Material:</p>

<ol>

  <li> Introduction</li>

  <ol type = "A">
    <li><a href = 01intro/history.pdf> The History of Deep Learning and Moore's Law of AI</a></li>

    <li><a href = 01intro/fundamentals.pdf> The Fundamental Equations of Deep Learning</a></li>

    <li><a href = 01intro/problems.pdf>Problems</a></li>

  </ol>

  <li> Frameworks and Back-Propagation</li>

  <ol type = "A">

    <li><a href = 02MLP/frameworks.pdf> Deep Learning Frameworks</a></li>

    <li><a href = 02MLP/Backprop.pdf> Backpropagation for Scalar Source Code</a></li>

    <li><a href = 02MLP/backprop2.pdf> Backpropagation for Tensor Source Code</a></li>

    <li><a href = 02MLP/minibatching.pdf>  Minibatching: The Batch Index</a></li>

    <li><a href = 02MLP/EDFslides.pdf>  The Educational Framework (EDF)</a></li>
        
    <li><a href = 02MLP/problems.pdf> Problems</a></li>

    <li><a href = 02MLP/edf.py> EDF source code</a> 150 lines of Python/NumPy</li>
    
    <li><a href = 02MLP/PS1.zip> MNIST in EDF problem set</a></li>

    <li><a href = https://pytorch.org/tutorials/ > PyTorch tutorial</a></li>

  </ol>

  <li>Vision: Convolutional Neural Networks (CNNs)</li>

  <ol type = "A">
    <li><a href = 03CNNs/Einstein.pdf> Einstein Notation</li>
    <li><a href = 03CNNs/CNNs.pdf> CNNs</li>
    <li><a href = 03CNNs/trainability.pdf> Trainability: Relu, Initialization, Batch Normalization and Residual Connections (ResNet)</li>
    <li><a href = 03CNNs/CNNb.html> Invariant Theory (optional)</li>
    <li><a href = 03CNNs/problems.pdf> Problems</a></li>
    <li><a href = https://pytorch.org/docs/stable/nn.functional.html?highlight=convolution>Pytorch Convolution Functions</a></li>
  </ol>

  <li> Natural Language Processing</li>

  <ol type = 'A'>

    <li><a href = 05Rnns/LangModels.pdf> Language Modeling</a></li>
    <li><a href = 05RNNs/RNNs.pdf> Recurrent Neural Networks (RNNs)</a></li>
    <li><a href = 05RNNs/Translation.pdf> Machine Translation and Attention</a></li>
    <li><a href = 05RNNs/Transformer.pdf> The Transformer</a></li>
    <li><a href = 05RNNs/Phrases.pdf> Statistical Machine Translaton (optional)</a></li>

    <li><a href = 05RNNs/problems.pdf>Problems</a></li>
    
    <li>References</li>

    <ol type = "i">
      <li><a href = https://arxiv.org/abs/1409.3215> Original sequence to sequence paper </a></li>

      <li><a href = https://arxiv.org/abs/1409.0473> Original attention paper </a></li>

      <li><a href = https://arxiv.org/abs/1611.04558> Google's Revolution in Machine Translation </a></li>
    
      <li><a href = https://arxiv.org/abs/1706.03762> Attention is all you need</a></li>
      </ol>

  </ol>

  <li><a href = 06SGD/SGD.html> Variants of Stochastic Gradient Descent (SGD)</li>

  <li><a href = 07regularization/regularization.html> Generalization and Regularization</li>

  <li><a href = 09GraphicalModels/CTC.html> Connectionist Temporal Classification (CTC)</li>

  <li><a href = 09GraphicalModels/DGMs.html> Deep Graphical Models</li>

  <li><a href = 08InfoTheory/information.html> More Information Theory: Perils of Differential Entropy </li>
  
  <li><a href = 14GANs/GANs.html> Generative Adversarial Networks (GANs)</a></li>

  <li><a href = 11AutoEncoders/Rate.html> Rate-Distortion Autoencoders (RDAs)</li>

  <li><a href = 11AutoEncoders/Variational.html> Variational Autoencoders (VAEs)</li>

  <li><a href = pretraining/pretraining.html> Pretraining</a></li>

  <li><a href = 15RL/RL.html> Reinforcement Learning (RL)</a></li>

  <li><a href = 16alpha/alpha.html> AlphaZero</a></li>

  <li><a href = 13SGD2/SGD2.html> Gradients as Dual Vectors, Hessian-Vector Products, and Information Geometry </a></li>

  <li><a href = 17Interpretation/Interp.html> The Black Box Problem</a></li>

  <li><a href = 18AGI/AGI.html> The Quest for Artificial General Intelligence (AGI)</a></li>
  
</ol>

<ol>
  <li><a href = quiz2-20/quiz2.pdf> Quiz 2</a></li>
  <li><a href = quiz3-20/quiz3.pdf> Quiz 3</a></li>
  <li><a href = quiz4/quiz4.pdf> Quiz 4</a></li>
  <li><a href = quiz5/quiz5.pdf> Quiz 5</a></li>
</ol>

<p> Each quiz will occupy the last 2/3 of one class.  The quizes are scheduled two weeks apart as follows: </p>

<ul>
  <li> Quiz 2, Tuesday January 28 covers lectures 3,4, and 5.</li>
  <li> Quiz 3, Tuesday February 11 covers lectures 6, 7 and 8 </li>
  <li> Quiz 4, Tuesday February 25 covers lectures 10, 11 and 12 </li>
  <li> Quiz 5, Tuesday March 10 </li>
</ul>

There is no final.

<p> Office hours will be Monday and Friday 2:00 to 4:00 in the library (TTIC 435).</p>

  


  


