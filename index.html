 <title>TTIC 31230: Fundamentals of Deep Learning</title>

<header>TTIC 31230: Fundamentals of Deep Learning</header>

<p> David McAllester</p>

<p> Winter 2020</p>

<p> TA: Adam Dziedzic</p>
<p> Grader: Gray Mackall </p>

<!-- <p style="color:red"> Last Lecture Canceled.  Prof. McAllester appears to have some mild illness and it seems best to err on the side of the safety of the students.</p> -->

<p> In 2020, as in 2019, there will be no machine problems or class projects.  The course will again be treated as an "algorithms class" rather than a "programming class".</p>

<p> The grade will be based entirely on five equally weighted in-class quizes.  Each lecture comes with paractice problems with solutions.
The quizes will include problems from the practice set --- you just need to memorize the solution --- as well as original problems.</p>

<p>Lectures Slides and Course Material:</p>

<ol>

  <li> Introduction</li>

  <ol>
    <li><a href = 01intro/history.pdf> Slides on The History of Deep Learning and Moore's Law of AI</a></li>

    <li><a href = 01intro/fundametnal.pdf> Slides on The Fundamental Equations of Deep Learning of Deep Learning</a></li>

    <li><a href = 01intro/problems.pdf>Problems</a></li>

  </ol>

  <li> Back-Propagation and Frameworks</li>

  <ol>

    <li><a href = 02MLP/MLP.pdf> Lecture Slides</a></li>
    
    <li><a href = 02MLP/problems.pdf> Problems</a></li>

    <li><a href = https://pytorch.org/tutorials/ > PyTorch tutorial</a></li>

    <li>The educational framework (EDF) <a href = edf.py> EDF source code</a> 150 lines of Python/NumPy</li>
    
    <li><a href = PS1.zip> MNIST in EDF problem set (from 2018 class)</a></li>
  </ol>

  <li><a href = 03CNNs/CNNs.html> Convolutional Neural Networks (CNNs)</li>

  <!-- <li><a href = 04Highway/highway.html> Trainability: Initialization, Batch Normalization, ResNet and Gated RNNs </li> -->

  <li><a href = 05RNNs/LangModel.html> Language Modeling, Recurrent Neural Networks (RNNs), Machine Translation and Attention</li>

  <li><a href = 06SGD/SGD.html> Variants of Stochastic Gradient Descent (SGD)</li>

  <li><a href = 07regularization/regularization.html> Generalization and Regularization</li>

  <li><a href = 09GraphicalModels/CTC.html> Connectionist Temporal Classification (CTC)</li>

  <li><a href = 09GraphicalModels/DGMs.html> Deep Graphical Models</li>

  <li><a href = 08InfoTheory/information.html> More Information Theory: Perils of Differential Entropy </li>
  
  <li><a href = 14GANs/GANs.html> Generative Adversarial Networks (GANs)</a></li>

  <li><a href = 11AutoEncoders/Rate.html> Rate-Distortion Autoencoders (RDAs)</li>

  <li><a href = 11AutoEncoders/Variational.html> Variational Autoencoders (VAEs)</li>

  <li><a href = pretraining/pretraining.html> Pretraining</a></li>

  <li><a href = 15RL/RL.html> Reinforcement Learning (RL)</a></li>

  <li><a href = 16alpha/alpha.html> AlphaZero</a></li>

  <li><a href = 13SGD2/SGD2.html> Gradients as Dual Vectors, Hessian-Vector Products, and Information Geometry </a></li>

  <li><a href = 17Interpretation/Interp.html> The Black Box Problem</a></li>

  <li><a href = 18AGI/AGI.html> The Quest for Artificial General Intelligence (AGI)</a></li>
  
</ol>

<ol>
  <li><a href = quiz1-20/quiz1-20.pdf> Quiz 1</a></li>
  <li><a href = quiz2-20/quiz2.pdf> Quiz 2</a></li>
  <li><a href = quiz3-20/quiz3.pdf> Quiz 3</a></li>
  <li><a href = quiz4/quiz4.pdf> Quiz 4</a></li>
  <li><a href = quiz5/quiz5.pdf> Quiz 5</a></li>
</ol>

<p> Each quiz will occupy the last 2/3 of one class.  The quizes are scheduled two weeks apart as follows: </p>

<ul>
  <li> Quiz 1, Tuesday January 14 covers lectures 1 and 2. </li>
  <li> Quiz 2, Tuesday January 28 covers lectures 3,4, and 5.</li>
  <li> Quiz 3, Tuesday February 11 covers lectures 6, 7 and 8 </li>
  <li> Quiz 4, Tuesday February 25 covers lectures 10, 11 and 12 </li>
  <li> Quiz 5, Tuesday March 10 </li>
</ul>

There is no final.

<p> Office hours will be Monday and Friday 2:00 to 4:00 in the library (TTIC 435).</p>

  


  


